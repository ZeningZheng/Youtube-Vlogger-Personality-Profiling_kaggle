{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65ac80f",
   "metadata": {
    "_cell_guid": "39fc8038-1e29-4bb3-8486-160aa629127b",
    "_execution_state": "idle",
    "_uuid": "047afe52-49c0-4445-bc38-8ce6b6825e9d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:29.165696Z",
     "iopub.status.busy": "2021-09-21T11:02:29.162334Z",
     "iopub.status.idle": "2021-09-21T11:02:31.898466Z",
     "shell.execute_reply": "2021-09-21T11:02:31.896591Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.777472,
     "end_time": "2021-09-21T11:02:31.898797",
     "exception": false,
     "start_time": "2021-09-21T11:02:29.121325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.4     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.2     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.7\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.4.0     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load libraries ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "library(tidyverse) # metapackage with lots of helpful functions\n",
    "library(tidytext)\n",
    "\n",
    "\n",
    "# list.files(path = \"../input\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd8910",
   "metadata": {
    "_cell_guid": "e96f7417-8cec-4390-aa25-b7fb7b0d137f",
    "_uuid": "dcb2b11f-1f84-42ed-bcea-e423337c4200",
    "papermill": {
     "duration": 0.035483,
     "end_time": "2021-09-21T11:02:31.970176",
     "exception": false,
     "start_time": "2021-09-21T11:02:31.934693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are three .csv files in the directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd078adc",
   "metadata": {
    "_cell_guid": "60fffa58-51df-4b85-8405-7208ac965554",
    "_uuid": "5a7e0e4d-bd75-4313-ad70-0df228feef48",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:32.100085Z",
     "iopub.status.busy": "2021-09-21T11:02:32.045880Z",
     "iopub.status.idle": "2021-09-21T11:02:32.130854Z",
     "shell.execute_reply": "2021-09-21T11:02:32.128530Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.125995,
     "end_time": "2021-09-21T11:02:32.131011",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.005016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory_content = list.files(\"../input/bda2021big5/youtube-personality\", full.names = TRUE)\n",
    "# print(directory_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7b398",
   "metadata": {
    "_cell_guid": "f8049998-2ebe-4324-8f2d-48050b8d95bc",
    "_uuid": "f4f5800e-e781-4c2b-a2da-8d42ea353a2d",
    "papermill": {
     "duration": 0.034606,
     "end_time": "2021-09-21T11:02:32.200082",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.165476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In addition there's a \"transcript\" folder (see number \\[2\\] in the output above) in which the actual video transcripts are stored in `.txt` files. \n",
    "\n",
    "Store these file paths in variables for easy reference later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3b348d",
   "metadata": {
    "_cell_guid": "94e0f55e-2f9a-4849-9bde-34feb94dc4f4",
    "_uuid": "a73254cd-f66d-4422-9d4c-2521f79b39fc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:32.278719Z",
     "iopub.status.busy": "2021-09-21T11:02:32.276083Z",
     "iopub.status.idle": "2021-09-21T11:02:32.307855Z",
     "shell.execute_reply": "2021-09-21T11:02:32.305391Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.072414,
     "end_time": "2021-09-21T11:02:32.308049",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.235635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the transcripts directory with transcript .txt files\n",
    "path_to_transcripts = directory_content[2] \n",
    "\n",
    "# .csv filenames (see output above)\n",
    "AudioVisual_file    = directory_content[3]\n",
    "Gender_file         = directory_content[4]\n",
    "Personality_file    = directory_content[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a160d31",
   "metadata": {
    "_cell_guid": "1e2591f6-6257-49b6-b8ca-5211d63b2c15",
    "_uuid": "5564aff0-1b9a-4ad0-8deb-c59822efb02f",
    "papermill": {
     "duration": 0.034398,
     "end_time": "2021-09-21T11:02:32.377305",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.342907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Import the data\n",
    "\n",
    "We'll import: Transcripts, Personality scores, Gender\n",
    "\n",
    "## 1.1 Importing transcripts\n",
    "\n",
    "The transcript text files are stored in the subfolder 'transcripts'. They can be listed with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9349ae4",
   "metadata": {
    "_cell_guid": "83284cd6-8b82-4e44-b327-a7bb30035ea1",
    "_uuid": "9bb0363f-c1e8-4e78-95e8-240628cd1c56",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:32.455123Z",
     "iopub.status.busy": "2021-09-21T11:02:32.452433Z",
     "iopub.status.idle": "2021-09-21T11:02:32.504818Z",
     "shell.execute_reply": "2021-09-21T11:02:32.502150Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.092561,
     "end_time": "2021-09-21T11:02:32.504995",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.412434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_files = list.files(path_to_transcripts, full.names = TRUE) \n",
    "\n",
    "# print(head(transcript_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0d500",
   "metadata": {
    "_cell_guid": "c87a60dd-6c43-4ed6-a35f-45d4efc47db6",
    "_uuid": "4e3cb8fd-83db-4698-a565-2115a202fe9b",
    "papermill": {
     "duration": 0.034339,
     "end_time": "2021-09-21T11:02:32.573942",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.539603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The transcript file names encode the vlogger ID that you will need for joining information from the different data frames. A clean way to extract the vlogger ID's from the names is by using the funcation `basename()` and removing the file extension \".txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48002658",
   "metadata": {
    "_cell_guid": "93382a21-dd15-4747-94b4-203131cbe705",
    "_uuid": "9a179032-bde8-462a-b223-89a45b1338d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:32.651422Z",
     "iopub.status.busy": "2021-09-21T11:02:32.648562Z",
     "iopub.status.idle": "2021-09-21T11:02:32.692137Z",
     "shell.execute_reply": "2021-09-21T11:02:32.689705Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.083693,
     "end_time": "2021-09-21T11:02:32.692307",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.608614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vlogId = basename(transcript_files)\n",
    "vlogId = str_replace(vlogId, pattern = \".txt$\", replacement = \"\")\n",
    "# head(vlogId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3367e9",
   "metadata": {
    "_cell_guid": "e5bd1c34-e28f-4b02-a1a5-c49037d8e40d",
    "_uuid": "a48356cc-bb9c-4edd-9cca-0082f5dfcdaf",
    "papermill": {
     "duration": 0.034454,
     "end_time": "2021-09-21T11:02:32.760993",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.726539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To include features extracted from the transcript texts you will have to read the text from files and store them in a data frame. For this, you will need the full file paths as stored in `transcript_files`.\n",
    "\n",
    "Here are some tips to do that programmatically\n",
    "\n",
    "- use either a `for` loop, the `sapply()` function, or the `map_chr()` from the `tidyverse`\n",
    "- don't forget to also store `vlogId` extracted with the code above \n",
    "\n",
    "We will use the `map_chr()` function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c2c704",
   "metadata": {
    "_cell_guid": "1553130b-5066-4d9b-8219-9c1ca9b3ba1a",
    "_uuid": "bb66ae39-9ef8-46fb-8971-082dc333bdb8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:32.838156Z",
     "iopub.status.busy": "2021-09-21T11:02:32.835262Z",
     "iopub.status.idle": "2021-09-21T11:02:34.335845Z",
     "shell.execute_reply": "2021-09-21T11:02:34.333103Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.541059,
     "end_time": "2021-09-21T11:02:34.336098",
     "exception": false,
     "start_time": "2021-09-21T11:02:32.795039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in readLines(.x):\n",
      "“incomplete final line found on '../input/bda2021big5/youtube-personality/transcripts/VLOG11.txt'”\n"
     ]
    }
   ],
   "source": [
    "transcripts_df = tibble(\n",
    "    \n",
    "    # vlogId connects each transcripts to a vlogger\n",
    "    vlogId=vlogId,\n",
    "    \n",
    "    # Read the transcript text from all file and store as a string\n",
    "    Text = map_chr(transcript_files, ~ paste(readLines(.x), collapse = \"\\\\n\")), \n",
    "    \n",
    "    # `filename` keeps track of the specific video transcript\n",
    "    filename = transcript_files\n",
    ")\n",
    "# head(transcripts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd8c2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:34.418508Z",
     "iopub.status.busy": "2021-09-21T11:02:34.415423Z",
     "iopub.status.idle": "2021-09-21T11:02:34.462117Z",
     "shell.execute_reply": "2021-09-21T11:02:34.459828Z"
    },
    "papermill": {
     "duration": 0.089423,
     "end_time": "2021-09-21T11:02:34.462270",
     "exception": false,
     "start_time": "2021-09-21T11:02:34.372847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>Text</th><th scope=col>filename</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>You know what I see - - no, more like hear a lot these days, is people calling other people gay as an insult. Now what makes people come up with calling others gay? Now here's an example. Hey, hey, you wanna trade Pokemon or Ziegfield cards? Or, or, or we can play, we can play superheroes. Oh, can I be Optimus Prime? Dude, you are so gay. Dude, the cool kids do crack. Oh, my mommy says, say no to drugs. Okay, how the hell does playing Pokemon cards or -- or --- or dancing or holding hands with another guy make me homosexual? I don't get these people. \\nThis is how it is in my school. Okay, here's an example. All right, um, when they see two guys are gay, they're together, they're like no, ew, no. No, no that -- that doesn't go together - - you know, two guys, no. two sticks, no. It just doesn't work like . But when they see two girls, they're like, get it on. And I don't get these people. I've never seen someone say like, oh, you're so homosexual or you're so lesbian or you're such a child molester. It is always the word gay, cause apparently gay is now an insult, even though the word means like happy and lively and that kinda giddy feeling you have inside, like -- -- but no you have to turn that happy word into a mean word. Apparently, we can do that now, turning good things into bad things. It's like how Spiderman felt good, but then that -- that -- that grease that gets all over him and then and then evil Dr. Octopus. That's so gay, you like Spiderman. Lar, I'm going to the movies with the guys to watch Mama Mia. \\nYou never know if other people are offended by what you say. I'm not saying you're a bad person if you do it. I used to do it all the time. I'm more focused on why we say it. In the end, we're all the same. You know, there's nothing wrong with it. I was just wondering where it all came from, you know. All right, thanks a lot for watching. Oh, yeah and the club channel is up and running. So, make sure to check that out because there's gonna be a lot of cool stuff on there. We'll do up to like four challenges at a time. We'll do contests, dares, questions. In the end, there's gonna be a lot of viewer interactions, so it's gonna be really fun. We may even put other people on the video too. So check it. </td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 3\n",
       "\\begin{tabular}{lll}\n",
       " vlogId & Text & filename\\\\\n",
       " <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & You know what I see - - no, more like hear a lot these days, is people calling other people gay as an insult. Now what makes people come up with calling others gay? Now here's an example. Hey, hey, you wanna trade Pokemon or Ziegfield cards? Or, or, or we can play, we can play superheroes. Oh, can I be Optimus Prime? Dude, you are so gay. Dude, the cool kids do crack. Oh, my mommy says, say no to drugs. Okay, how the hell does playing Pokemon cards or -- or --- or dancing or holding hands with another guy make me homosexual? I don't get these people. \\textbackslash{}nThis is how it is in my school. Okay, here's an example. All right, um, when they see two guys are gay, they're together, they're like no, ew, no. No, no that -- that doesn't go together - - you know, two guys, no. two sticks, no. It just doesn't work like . But when they see two girls, they're like, get it on. And I don't get these people. I've never seen someone say like, oh, you're so homosexual or you're so lesbian or you're such a child molester. It is always the word gay, cause apparently gay is now an insult, even though the word means like happy and lively and that kinda giddy feeling you have inside, like -- -- but no you have to turn that happy word into a mean word. Apparently, we can do that now, turning good things into bad things. It's like how Spiderman felt good, but then that -- that -- that grease that gets all over him and then and then evil Dr. Octopus. That's so gay, you like Spiderman. Lar, I'm going to the movies with the guys to watch Mama Mia. \\textbackslash{}nYou never know if other people are offended by what you say. I'm not saying you're a bad person if you do it. I used to do it all the time. I'm more focused on why we say it. In the end, we're all the same. You know, there's nothing wrong with it. I was just wondering where it all came from, you know. All right, thanks a lot for watching. Oh, yeah and the club channel is up and running. So, make sure to check that out because there's gonna be a lot of cool stuff on there. We'll do up to like four challenges at a time. We'll do contests, dares, questions. In the end, there's gonna be a lot of viewer interactions, so it's gonna be really fun. We may even put other people on the video too. So check it.  & ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 3\n",
       "\n",
       "| vlogId &lt;chr&gt; | Text &lt;chr&gt; | filename &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| VLOG1 | You know what I see - - no, more like hear a lot these days, is people calling other people gay as an insult. Now what makes people come up with calling others gay? Now here's an example. Hey, hey, you wanna trade Pokemon or Ziegfield cards? Or, or, or we can play, we can play superheroes. Oh, can I be Optimus Prime? Dude, you are so gay. Dude, the cool kids do crack. Oh, my mommy says, say no to drugs. Okay, how the hell does playing Pokemon cards or -- or --- or dancing or holding hands with another guy make me homosexual? I don't get these people. \\nThis is how it is in my school. Okay, here's an example. All right, um, when they see two guys are gay, they're together, they're like no, ew, no. No, no that -- that doesn't go together - - you know, two guys, no. two sticks, no. It just doesn't work like . But when they see two girls, they're like, get it on. And I don't get these people. I've never seen someone say like, oh, you're so homosexual or you're so lesbian or you're such a child molester. It is always the word gay, cause apparently gay is now an insult, even though the word means like happy and lively and that kinda giddy feeling you have inside, like -- -- but no you have to turn that happy word into a mean word. Apparently, we can do that now, turning good things into bad things. It's like how Spiderman felt good, but then that -- that -- that grease that gets all over him and then and then evil Dr. Octopus. That's so gay, you like Spiderman. Lar, I'm going to the movies with the guys to watch Mama Mia. \\nYou never know if other people are offended by what you say. I'm not saying you're a bad person if you do it. I used to do it all the time. I'm more focused on why we say it. In the end, we're all the same. You know, there's nothing wrong with it. I was just wondering where it all came from, you know. All right, thanks a lot for watching. Oh, yeah and the club channel is up and running. So, make sure to check that out because there's gonna be a lot of cool stuff on there. We'll do up to like four challenges at a time. We'll do contests, dares, questions. In the end, there's gonna be a lot of viewer interactions, so it's gonna be really fun. We may even put other people on the video too. So check it.  | ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId\n",
       "1 VLOG1 \n",
       "  Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1 You know what I see - - no, more like hear a lot these days, is people calling other people gay as an insult. Now what makes people come up with calling others gay? Now here's an example. Hey, hey, you wanna trade Pokemon or Ziegfield cards? Or, or, or we can play, we can play superheroes. Oh, can I be Optimus Prime? Dude, you are so gay. Dude, the cool kids do crack. Oh, my mommy says, say no to drugs. Okay, how the hell does playing Pokemon cards or -- or --- or dancing or holding hands with another guy make me homosexual? I don't get these people. \\\\nThis is how it is in my school. Okay, here's an example. All right, um, when they see two guys are gay, they're together, they're like no, ew, no. No, no that -- that doesn't go together - - you know, two guys, no. two sticks, no. It just doesn't work like . But when they see two girls, they're like, get it on. And I don't get these people. I've never seen someone say like, oh, you're so homosexual or you're so lesbian or you're such a child molester. It is always the word gay, cause apparently gay is now an insult, even though the word means like happy and lively and that kinda giddy feeling you have inside, like -- -- but no you have to turn that happy word into a mean word. Apparently, we can do that now, turning good things into bad things. It's like how Spiderman felt good, but then that -- that -- that grease that gets all over him and then and then evil Dr. Octopus. That's so gay, you like Spiderman. Lar, I'm going to the movies with the guys to watch Mama Mia. \\\\nYou never know if other people are offended by what you say. I'm not saying you're a bad person if you do it. I used to do it all the time. I'm more focused on why we say it. In the end, we're all the same. You know, there's nothing wrong with it. I was just wondering where it all came from, you know. All right, thanks a lot for watching. Oh, yeah and the club channel is up and running. So, make sure to check that out because there's gonna be a lot of cool stuff on there. We'll do up to like four challenges at a time. We'll do contests, dares, questions. In the end, there's gonna be a lot of viewer interactions, so it's gonna be really fun. We may even put other people on the video too. So check it. \n",
       "  filename                                                      \n",
       "1 ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcripts_df %>% \n",
    "    head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a966ce9",
   "metadata": {
    "_cell_guid": "6a33e296-2dbb-4aff-b538-a664c3b0c5ea",
    "_uuid": "71cac44a-ef63-4905-9301-3cac5bdb4853",
    "execution": {
     "iopub.execute_input": "2021-09-20T16:22:02.773731Z",
     "iopub.status.busy": "2021-09-20T16:22:02.771306Z",
     "iopub.status.idle": "2021-09-20T16:22:02.789965Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035781,
     "end_time": "2021-09-21T11:02:34.533911",
     "exception": false,
     "start_time": "2021-09-21T11:02:34.498130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Import personality scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496aaccd",
   "metadata": {
    "_cell_guid": "3b23f306-d346-457a-86e5-e567b85fc8c0",
    "_uuid": "a4f9c3b5-7277-4b3f-838e-e51d3acd321e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:34.614115Z",
     "iopub.status.busy": "2021-09-21T11:02:34.611219Z",
     "iopub.status.idle": "2021-09-21T11:02:34.828466Z",
     "shell.execute_reply": "2021-09-21T11:02:34.827048Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.259095,
     "end_time": "2021-09-21T11:02:34.828616",
     "exception": false,
     "start_time": "2021-09-21T11:02:34.569521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36m──\u001b[39m \u001b[1m\u001b[1mColumn specification\u001b[1m\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "cols(\n",
      "  vlogId = \u001b[31mcol_character()\u001b[39m,\n",
      "  Extr = \u001b[32mcol_double()\u001b[39m,\n",
      "  Agr = \u001b[32mcol_double()\u001b[39m,\n",
      "  Cons = \u001b[32mcol_double()\u001b[39m,\n",
      "  Emot = \u001b[32mcol_double()\u001b[39m,\n",
      "  Open = \u001b[32mcol_double()\u001b[39m\n",
      ")\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>4.9</td><td>3.7</td><td>3.6</td><td>3.2</td><td>5.5</td></tr>\n",
       "\t<tr><td>VLOG3</td><td>5.0</td><td>5.0</td><td>4.6</td><td>5.3</td><td>4.4</td></tr>\n",
       "\t<tr><td>VLOG5</td><td>5.9</td><td>5.3</td><td>5.3</td><td>5.8</td><td>5.5</td></tr>\n",
       "\t<tr><td>VLOG6</td><td>5.4</td><td>4.8</td><td>4.4</td><td>4.8</td><td>5.7</td></tr>\n",
       "\t<tr><td>VLOG7</td><td>4.7</td><td>5.1</td><td>4.4</td><td>5.1</td><td>4.7</td></tr>\n",
       "\t<tr><td>VLOG9</td><td>5.6</td><td>5.0</td><td>4.0</td><td>4.2</td><td>4.9</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " vlogId & Extr & Agr & Cons & Emot & Open\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & 4.9 & 3.7 & 3.6 & 3.2 & 5.5\\\\\n",
       "\t VLOG3 & 5.0 & 5.0 & 4.6 & 5.3 & 4.4\\\\\n",
       "\t VLOG5 & 5.9 & 5.3 & 5.3 & 5.8 & 5.5\\\\\n",
       "\t VLOG6 & 5.4 & 4.8 & 4.4 & 4.8 & 5.7\\\\\n",
       "\t VLOG7 & 4.7 & 5.1 & 4.4 & 5.1 & 4.7\\\\\n",
       "\t VLOG9 & 5.6 & 5.0 & 4.0 & 4.2 & 4.9\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 6\n",
       "\n",
       "| vlogId &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| VLOG1 | 4.9 | 3.7 | 3.6 | 3.2 | 5.5 |\n",
       "| VLOG3 | 5.0 | 5.0 | 4.6 | 5.3 | 4.4 |\n",
       "| VLOG5 | 5.9 | 5.3 | 5.3 | 5.8 | 5.5 |\n",
       "| VLOG6 | 5.4 | 4.8 | 4.4 | 4.8 | 5.7 |\n",
       "| VLOG7 | 4.7 | 5.1 | 4.4 | 5.1 | 4.7 |\n",
       "| VLOG9 | 5.6 | 5.0 | 4.0 | 4.2 | 4.9 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId Extr Agr Cons Emot Open\n",
       "1 VLOG1  4.9  3.7 3.6  3.2  5.5 \n",
       "2 VLOG3  5.0  5.0 4.6  5.3  4.4 \n",
       "3 VLOG5  5.9  5.3 5.3  5.8  5.5 \n",
       "4 VLOG6  5.4  4.8 4.4  4.8  5.7 \n",
       "5 VLOG7  4.7  5.1 4.4  5.1  4.7 \n",
       "6 VLOG9  5.6  5.0 4.0  4.2  4.9 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the Personality scores\n",
    "pers_df = read_delim(Personality_file, delim=\" \")\n",
    "head(pers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3d7ff",
   "metadata": {
    "_cell_guid": "0f1b8dec-eccb-4fe8-964d-f6ee2a2bfa2e",
    "_uuid": "483309a5-f1d9-4c52-99f6-7981af701b15",
    "papermill": {
     "duration": 0.0384,
     "end_time": "2021-09-21T11:02:34.910438",
     "exception": false,
     "start_time": "2021-09-21T11:02:34.872038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import gender\n",
    "\n",
    "Gender info is stored in a separate `.csv` which is also delimited with a space. This file doesn't have column names, so we have to add them ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380c40bd",
   "metadata": {
    "_cell_guid": "4addc5a4-5b6f-4fbe-a84f-7ee5733a8271",
    "_uuid": "42642c23-d42f-4265-afd8-e03477f0ec07",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:35.004576Z",
     "iopub.status.busy": "2021-09-21T11:02:35.002504Z",
     "iopub.status.idle": "2021-09-21T11:02:35.028578Z",
     "shell.execute_reply": "2021-09-21T11:02:35.027019Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.076312,
     "end_time": "2021-09-21T11:02:35.028741",
     "exception": false,
     "start_time": "2021-09-21T11:02:34.952429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_df = read.delim(Gender_file, head = FALSE, sep= \" \", skip = 2)\n",
    "\n",
    "# Add column names\n",
    "names(gender_df) = c('vlogId', 'gender')\n",
    "# head(gender_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e266aef",
   "metadata": {
    "_cell_guid": "252e73f9-ed0f-4161-a6e8-f287e6dfaddc",
    "_uuid": "4093194b-da93-4b07-996f-82c1fe810418",
    "papermill": {
     "duration": 0.043438,
     "end_time": "2021-09-21T11:02:35.113765",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.070327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merging the `gender` and `pers` dataframes\n",
    "\n",
    "Obviously, we want all the information in a single tidy data frame. While the builtin R function `merge()` can do that, the `tidyverse()` has a number of more versatile and consistent functions called `left_join`, `right_join`, `inner_join`, `outer_join`, and `anti_join`. We'll use `left_join` here to merge the gender and personality data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "911f7586",
   "metadata": {
    "_cell_guid": "15d7bd85-f197-4803-8daa-6193f7929978",
    "_uuid": "58eb0582-3c68-458e-a807-187ef63fd52a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:35.193917Z",
     "iopub.status.busy": "2021-09-21T11:02:35.191948Z",
     "iopub.status.idle": "2021-09-21T11:02:35.226332Z",
     "shell.execute_reply": "2021-09-21T11:02:35.224749Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.076016,
     "end_time": "2021-09-21T11:02:35.226490",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.150474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"vlogId\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using left-join therefore any pers that do not have a corresponding gender score will be deleted \n",
    "\n",
    "vlogger_df <- left_join(gender_df, pers_df)\n",
    "# head(vlogger_df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775a3b6",
   "metadata": {
    "_cell_guid": "5ef44461-656c-4d24-8e4f-c137159fbc16",
    "_uuid": "d99b97d2-a621-4347-bde2-39e316bdf494",
    "papermill": {
     "duration": 0.036743,
     "end_time": "2021-09-21T11:02:35.301840",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.265097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that some rows, like row 5, has `NA`'s for the personality scores. This is because this row corresponds to the vlogger with vlogId `VLOG8` is part of the test set. You still have to split `vlogger_df` into the training and test set, as shown below.\n",
    "\n",
    "We leave the `transcripts_df` data frame seperate for now, because you will first have to extract features from the transcripts first. Once you have those features in a tidy data frame, including a `vlogId` column, you can refer to this `left_join` example to merge your features with `vlogger_df` in one single tidy data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4e469",
   "metadata": {
    "_cell_guid": "b3b16406-5f3b-45c5-b1a9-9d2f98d66210",
    "_uuid": "cda7cf6b-3680-4451-b1c5-6e276d2d5430",
    "papermill": {
     "duration": 0.036663,
     "end_time": "2021-09-21T11:02:35.375568",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.338905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Feature extraction from transcript texts\n",
    "### Tokenizing the text\n",
    "We have \"built\" 3 features ourselves, used one new word-list after exploring the literature and used two of the word-lists already given to us. In total, we have 6 predictors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6042395",
   "metadata": {
    "_cell_guid": "f80d89e3-c233-4a5d-ba0c-7f1ca9fb1b04",
    "_uuid": "1540d512-f479-4475-aaa9-45c3acae5178",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:35.456355Z",
     "iopub.status.busy": "2021-09-21T11:02:35.454833Z",
     "iopub.status.idle": "2021-09-21T11:02:35.615542Z",
     "shell.execute_reply": "2021-09-21T11:02:35.613723Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.203053,
     "end_time": "2021-09-21T11:02:35.615710",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.412657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>filename</th><th scope=col>word</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG99</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt</td><td>my     </td></tr>\n",
       "\t<tr><td>VLOG99</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt</td><td>life   </td></tr>\n",
       "\t<tr><td>VLOG99</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt</td><td>i'll   </td></tr>\n",
       "\t<tr><td>VLOG99</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt</td><td>keep   </td></tr>\n",
       "\t<tr><td>VLOG99</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt</td><td>you    </td></tr>\n",
       "\t<tr><td>VLOG99</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt</td><td>updated</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " vlogId & filename & word\\\\\n",
       " <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t VLOG99 & ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt & my     \\\\\n",
       "\t VLOG99 & ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt & life   \\\\\n",
       "\t VLOG99 & ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt & i'll   \\\\\n",
       "\t VLOG99 & ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt & keep   \\\\\n",
       "\t VLOG99 & ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt & you    \\\\\n",
       "\t VLOG99 & ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt & updated\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 3\n",
       "\n",
       "| vlogId &lt;chr&gt; | filename &lt;chr&gt; | word &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| VLOG99 | ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt | my      |\n",
       "| VLOG99 | ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt | life    |\n",
       "| VLOG99 | ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt | i'll    |\n",
       "| VLOG99 | ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt | keep    |\n",
       "| VLOG99 | ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt | you     |\n",
       "| VLOG99 | ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt | updated |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId filename                                                       \n",
       "1 VLOG99 ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt\n",
       "2 VLOG99 ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt\n",
       "3 VLOG99 ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt\n",
       "4 VLOG99 ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt\n",
       "5 VLOG99 ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt\n",
       "6 VLOG99 ../input/bda2021big5/youtube-personality/transcripts/VLOG99.txt\n",
       "  word   \n",
       "1 my     \n",
       "2 life   \n",
       "3 i'll   \n",
       "4 keep   \n",
       "5 you    \n",
       "6 updated"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# splitting all the sentences into tokens (words)\n",
    "\n",
    "transcripts_unnest <- \n",
    "transcripts_df %>%\n",
    "  unnest_tokens(word, Text, token = 'words')\n",
    "\n",
    "tail(transcripts_unnest) # check whether tokenizing worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9d99891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:35.701514Z",
     "iopub.status.busy": "2021-09-21T11:02:35.699518Z",
     "iopub.status.idle": "2021-09-21T11:02:35.727322Z",
     "shell.execute_reply": "2021-09-21T11:02:35.725637Z"
    },
    "papermill": {
     "duration": 0.072306,
     "end_time": "2021-09-21T11:02:35.727476",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.655170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>filename</th><th scope=col>word</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt</td><td>you </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt</td><td>know</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt</td><td>what</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt</td><td>i   </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt</td><td>see </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt</td><td>no  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " vlogId & filename & word\\\\\n",
       " <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt & you \\\\\n",
       "\t VLOG1 & ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt & know\\\\\n",
       "\t VLOG1 & ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt & what\\\\\n",
       "\t VLOG1 & ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt & i   \\\\\n",
       "\t VLOG1 & ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt & see \\\\\n",
       "\t VLOG1 & ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt & no  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 3\n",
       "\n",
       "| vlogId &lt;chr&gt; | filename &lt;chr&gt; | word &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| VLOG1 | ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt | you  |\n",
       "| VLOG1 | ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt | know |\n",
       "| VLOG1 | ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt | what |\n",
       "| VLOG1 | ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt | i    |\n",
       "| VLOG1 | ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt | see  |\n",
       "| VLOG1 | ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt | no   |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId filename                                                       word\n",
       "1 VLOG1  ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt you \n",
       "2 VLOG1  ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt know\n",
       "3 VLOG1  ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt what\n",
       "4 VLOG1  ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt i   \n",
       "5 VLOG1  ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt see \n",
       "6 VLOG1  ../input/bda2021big5/youtube-personality/transcripts/VLOG1.txt no  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# removing filename column \n",
    "head(transcripts_unnest) # still has file name\n",
    "transcripts_unnest <- transcripts_unnest[, -2] # remove filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712444a4",
   "metadata": {
    "papermill": {
     "duration": 0.038579,
     "end_time": "2021-09-21T11:02:35.807396",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.768817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature 1: Counting number of word occurences \n",
    "It is hypothesized that the number of words a vlogger uses predicts the vloggers personality. \n",
    "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073791\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ef7fc0",
   "metadata": {
    "_cell_guid": "f80d89e3-c233-4a5d-ba0c-7f1ca9fb1b04",
    "_uuid": "1540d512-f479-4475-aaa9-45c3acae5178",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:35.892613Z",
     "iopub.status.busy": "2021-09-21T11:02:35.890496Z",
     "iopub.status.idle": "2021-09-21T11:02:35.954264Z",
     "shell.execute_reply": "2021-09-21T11:02:35.952608Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.107582,
     "end_time": "2021-09-21T11:02:35.954413",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.846831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>n_words</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1  </td><td> 435</td></tr>\n",
       "\t<tr><td>VLOG10 </td><td> 449</td></tr>\n",
       "\t<tr><td>VLOG100</td><td> 293</td></tr>\n",
       "\t<tr><td>VLOG102</td><td>1346</td></tr>\n",
       "\t<tr><td>VLOG103</td><td> 769</td></tr>\n",
       "\t<tr><td>VLOG104</td><td> 788</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " vlogId & n\\_words\\\\\n",
       " <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t VLOG1   &  435\\\\\n",
       "\t VLOG10  &  449\\\\\n",
       "\t VLOG100 &  293\\\\\n",
       "\t VLOG102 & 1346\\\\\n",
       "\t VLOG103 &  769\\\\\n",
       "\t VLOG104 &  788\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 2\n",
       "\n",
       "| vlogId &lt;chr&gt; | n_words &lt;int&gt; |\n",
       "|---|---|\n",
       "| VLOG1   |  435 |\n",
       "| VLOG10  |  449 |\n",
       "| VLOG100 |  293 |\n",
       "| VLOG102 | 1346 |\n",
       "| VLOG103 |  769 |\n",
       "| VLOG104 |  788 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  n_words\n",
       "1 VLOG1    435   \n",
       "2 VLOG10   449   \n",
       "3 VLOG100  293   \n",
       "4 VLOG102 1346   \n",
       "5 VLOG103  769   \n",
       "6 VLOG104  788   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count word occurences \n",
    "\n",
    "word_count <-   \n",
    "  transcripts_unnest %>% count(vlogId) %>%  # count tokens\n",
    "  rename(n_words = n) # changing column names\n",
    "\n",
    "head(word_count) # check the feature word count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d362875",
   "metadata": {
    "papermill": {
     "duration": 0.039337,
     "end_time": "2021-09-21T11:02:36.034628",
     "exception": false,
     "start_time": "2021-09-21T11:02:35.995291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature 2: Counting average word length \n",
    "It is hypothesized that the average length of words vloggers use predicts their personality. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4274467f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:36.136289Z",
     "iopub.status.busy": "2021-09-21T11:02:36.134088Z",
     "iopub.status.idle": "2021-09-21T11:02:36.441230Z",
     "shell.execute_reply": "2021-09-21T11:02:36.438762Z"
    },
    "papermill": {
     "duration": 0.36279,
     "end_time": "2021-09-21T11:02:36.441470",
     "exception": false,
     "start_time": "2021-09-21T11:02:36.078680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>word_length</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG1  </td><td>3.903448</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG10 </td><td>4.552339</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG100</td><td>3.767918</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG102</td><td>3.971768</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG103</td><td>3.721717</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG104</td><td>4.317259</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & vlogId & word\\_length\\\\\n",
       "  & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG1   & 3.903448\\\\\n",
       "\t2 & VLOG10  & 4.552339\\\\\n",
       "\t3 & VLOG100 & 3.767918\\\\\n",
       "\t4 & VLOG102 & 3.971768\\\\\n",
       "\t5 & VLOG103 & 3.721717\\\\\n",
       "\t6 & VLOG104 & 4.317259\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | word_length &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | VLOG1   | 3.903448 |\n",
       "| 2 | VLOG10  | 4.552339 |\n",
       "| 3 | VLOG100 | 3.767918 |\n",
       "| 4 | VLOG102 | 3.971768 |\n",
       "| 5 | VLOG103 | 3.721717 |\n",
       "| 6 | VLOG104 | 4.317259 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  word_length\n",
       "1 VLOG1   3.903448   \n",
       "2 VLOG10  4.552339   \n",
       "3 VLOG100 3.767918   \n",
       "4 VLOG102 3.971768   \n",
       "5 VLOG103 3.721717   \n",
       "6 VLOG104 4.317259   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature extraction: counting average length of word \n",
    "\n",
    "length <- nchar(transcripts_unnest$word) # counting number of characters in each word\n",
    "\n",
    "word_length <- cbind(transcripts_unnest[, 1], length) # matrix with word_length included \n",
    " \n",
    "# calculate average word length \n",
    "word_length <- \n",
    "    word_length[, 2] %>%\n",
    "    aggregate(list(word_length$vlogId), mean) %>% # calculating average word-length for each group \n",
    "  rename(word_length = x, vlogId = Group.1) # changing the column names so we can join them to another table by vlogId later \n",
    "head(word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e101d3",
   "metadata": {
    "papermill": {
     "duration": 0.041442,
     "end_time": "2021-09-21T11:02:36.526197",
     "exception": false,
     "start_time": "2021-09-21T11:02:36.484755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Empathy and Distress\n",
    "These wordlists are found to relate to personality. \n",
    "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c956795b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:36.616665Z",
     "iopub.status.busy": "2021-09-21T11:02:36.614356Z",
     "iopub.status.idle": "2021-09-21T11:02:36.732369Z",
     "shell.execute_reply": "2021-09-21T11:02:36.730880Z"
    },
    "papermill": {
     "duration": 0.164154,
     "end_time": "2021-09-21T11:02:36.732560",
     "exception": false,
     "start_time": "2021-09-21T11:02:36.568406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>word</th><th scope=col>rating</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>helps      </td><td>4.315954</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>uncommon   </td><td>2.534964</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>blank      </td><td>3.559863</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>iraqis     </td><td>5.446981</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>explored   </td><td>4.401998</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>concentrate</td><td>3.637599</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & word & rating\\\\\n",
       "  & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & helps       & 4.315954\\\\\n",
       "\t2 & uncommon    & 2.534964\\\\\n",
       "\t3 & blank       & 3.559863\\\\\n",
       "\t4 & iraqis      & 5.446981\\\\\n",
       "\t5 & explored    & 4.401998\\\\\n",
       "\t6 & concentrate & 3.637599\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | word &lt;chr&gt; | rating &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | helps       | 4.315954 |\n",
       "| 2 | uncommon    | 2.534964 |\n",
       "| 3 | blank       | 3.559863 |\n",
       "| 4 | iraqis      | 5.446981 |\n",
       "| 5 | explored    | 4.401998 |\n",
       "| 6 | concentrate | 3.637599 |\n",
       "\n"
      ],
      "text/plain": [
       "  word        rating  \n",
       "1 helps       4.315954\n",
       "2 uncommon    2.534964\n",
       "3 blank       3.559863\n",
       "4 iraqis      5.446981\n",
       "5 explored    4.401998\n",
       "6 concentrate 3.637599"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>word</th><th scope=col>rating</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>helps      </td><td>2.409573</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>uncommon   </td><td>1.303017</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>blank      </td><td>3.729931</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>iraqis     </td><td>5.585402</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>explored   </td><td>3.672773</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>concentrate</td><td>2.681559</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & word & rating\\\\\n",
       "  & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & helps       & 2.409573\\\\\n",
       "\t2 & uncommon    & 1.303017\\\\\n",
       "\t3 & blank       & 3.729931\\\\\n",
       "\t4 & iraqis      & 5.585402\\\\\n",
       "\t5 & explored    & 3.672773\\\\\n",
       "\t6 & concentrate & 2.681559\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | word &lt;chr&gt; | rating &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | helps       | 2.409573 |\n",
       "| 2 | uncommon    | 1.303017 |\n",
       "| 3 | blank       | 3.729931 |\n",
       "| 4 | iraqis      | 5.585402 |\n",
       "| 5 | explored    | 3.672773 |\n",
       "| 6 | concentrate | 2.681559 |\n",
       "\n"
      ],
      "text/plain": [
       "  word        rating  \n",
       "1 helps       2.409573\n",
       "2 uncommon    1.303017\n",
       "3 blank       3.729931\n",
       "4 iraqis      5.585402\n",
       "5 explored    3.672773\n",
       "6 concentrate 2.681559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load empathy and the stress word lists\n",
    "empathy_data = read.table(\"../input/empathy-lexicon/empathy_lexicon.txt\", header = TRUE, sep = ',')\n",
    "distress_data = read.table(\"../input/distress-lexicon/distress_lexicon.txt\", header = TRUE, sep = ',')\n",
    "\n",
    "head(empathy_data)\n",
    "head(distress_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fcce7",
   "metadata": {
    "papermill": {
     "duration": 0.057977,
     "end_time": "2021-09-21T11:02:36.844946",
     "exception": false,
     "start_time": "2021-09-21T11:02:36.786969",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71990ddd",
   "metadata": {
    "papermill": {
     "duration": 0.043841,
     "end_time": "2021-09-21T11:02:36.939340",
     "exception": false,
     "start_time": "2021-09-21T11:02:36.895499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature 3: Empathy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74bb98a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:37.028719Z",
     "iopub.status.busy": "2021-09-21T11:02:37.028058Z",
     "iopub.status.idle": "2021-09-21T11:02:37.438091Z",
     "shell.execute_reply": "2021-09-21T11:02:37.436523Z"
    },
    "papermill": {
     "duration": 0.457158,
     "end_time": "2021-09-21T11:02:37.438244",
     "exception": false,
     "start_time": "2021-09-21T11:02:36.981086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>empathy</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG1  </td><td>3.060428</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG10 </td><td>3.261867</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG100</td><td>3.332329</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG102</td><td>3.123552</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG103</td><td>3.130718</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG104</td><td>3.166712</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & vlogId & empathy\\\\\n",
       "  & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG1   & 3.060428\\\\\n",
       "\t2 & VLOG10  & 3.261867\\\\\n",
       "\t3 & VLOG100 & 3.332329\\\\\n",
       "\t4 & VLOG102 & 3.123552\\\\\n",
       "\t5 & VLOG103 & 3.130718\\\\\n",
       "\t6 & VLOG104 & 3.166712\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | empathy &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | VLOG1   | 3.060428 |\n",
       "| 2 | VLOG10  | 3.261867 |\n",
       "| 3 | VLOG100 | 3.332329 |\n",
       "| 4 | VLOG102 | 3.123552 |\n",
       "| 5 | VLOG103 | 3.130718 |\n",
       "| 6 | VLOG104 | 3.166712 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  empathy \n",
       "1 VLOG1   3.060428\n",
       "2 VLOG10  3.261867\n",
       "3 VLOG100 3.332329\n",
       "4 VLOG102 3.123552\n",
       "5 VLOG103 3.130718\n",
       "6 VLOG104 3.166712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do an inner join an essay token data frame and empathy word list\n",
    "empathy <- \n",
    "    inner_join(transcripts_unnest, empathy_data, by = 'word') \n",
    "\n",
    "#count the empathy score\n",
    "\n",
    "empathy <- \n",
    "    empathy[, 3] %>%\n",
    "    aggregate(list(empathy$vlogId), mean) \n",
    "\n",
    "\n",
    "empathy  <- empathy  %>% # changing the column names so we can join them to another table by vlogId later \n",
    "  rename(empathy = rating, vlogId = Group.1)\n",
    "\n",
    "head(empathy) # check the feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903519b",
   "metadata": {
    "papermill": {
     "duration": 0.042,
     "end_time": "2021-09-21T11:02:37.523070",
     "exception": false,
     "start_time": "2021-09-21T11:02:37.481070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature 4:  Distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9e8c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:37.613207Z",
     "iopub.status.busy": "2021-09-21T11:02:37.611722Z",
     "iopub.status.idle": "2021-09-21T11:02:37.786434Z",
     "shell.execute_reply": "2021-09-21T11:02:37.784873Z"
    },
    "papermill": {
     "duration": 0.221281,
     "end_time": "2021-09-21T11:02:37.786588",
     "exception": false,
     "start_time": "2021-09-21T11:02:37.565307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>distress</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG1  </td><td>2.920563</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG10 </td><td>3.135499</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG100</td><td>2.980318</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG102</td><td>2.980737</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG103</td><td>2.970042</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG104</td><td>2.914677</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & vlogId & distress\\\\\n",
       "  & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG1   & 2.920563\\\\\n",
       "\t2 & VLOG10  & 3.135499\\\\\n",
       "\t3 & VLOG100 & 2.980318\\\\\n",
       "\t4 & VLOG102 & 2.980737\\\\\n",
       "\t5 & VLOG103 & 2.970042\\\\\n",
       "\t6 & VLOG104 & 2.914677\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | distress &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | VLOG1   | 2.920563 |\n",
       "| 2 | VLOG10  | 3.135499 |\n",
       "| 3 | VLOG100 | 2.980318 |\n",
       "| 4 | VLOG102 | 2.980737 |\n",
       "| 5 | VLOG103 | 2.970042 |\n",
       "| 6 | VLOG104 | 2.914677 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  distress\n",
       "1 VLOG1   2.920563\n",
       "2 VLOG10  3.135499\n",
       "3 VLOG100 2.980318\n",
       "4 VLOG102 2.980737\n",
       "5 VLOG103 2.970042\n",
       "6 VLOG104 2.914677"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do an inner join an essay token data frame and distress word list\n",
    "distress = \n",
    "    inner_join(transcripts_unnest, distress_data, by = 'word') \n",
    "\n",
    "#count the distress score\n",
    "\n",
    "distress <- \n",
    "    distress[, 3] %>%\n",
    "    aggregate(list(distress$vlogId), mean) \n",
    "\n",
    "distress  <- distress  %>% # changing the column names so we can join them to another table by vlogId later \n",
    "  rename(distress = rating, vlogId = Group.1)\n",
    "\n",
    "head(distress) #check the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab811b",
   "metadata": {
    "papermill": {
     "duration": 0.042758,
     "end_time": "2021-09-21T11:02:37.872720",
     "exception": false,
     "start_time": "2021-09-21T11:02:37.829962",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1e4abe8",
   "metadata": {
    "papermill": {
     "duration": 0.042408,
     "end_time": "2021-09-21T11:02:37.958586",
     "exception": false,
     "start_time": "2021-09-21T11:02:37.916178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature 5: Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b908f503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:38.050071Z",
     "iopub.status.busy": "2021-09-21T11:02:38.048287Z",
     "iopub.status.idle": "2021-09-21T11:02:38.294990Z",
     "shell.execute_reply": "2021-09-21T11:02:38.293442Z"
    },
    "papermill": {
     "duration": 0.294018,
     "end_time": "2021-09-21T11:02:38.295355",
     "exception": false,
     "start_time": "2021-09-21T11:02:38.001337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>total_adjective</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG1  </td><td> 44</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG10 </td><td> 50</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG100</td><td> 21</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG102</td><td>106</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG103</td><td> 66</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG104</td><td> 79</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & vlogId & total\\_adjective\\\\\n",
       "  & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG1   &  44\\\\\n",
       "\t2 & VLOG10  &  50\\\\\n",
       "\t3 & VLOG100 &  21\\\\\n",
       "\t4 & VLOG102 & 106\\\\\n",
       "\t5 & VLOG103 &  66\\\\\n",
       "\t6 & VLOG104 &  79\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | total_adjective &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| 1 | VLOG1   |  44 |\n",
       "| 2 | VLOG10  |  50 |\n",
       "| 3 | VLOG100 |  21 |\n",
       "| 4 | VLOG102 | 106 |\n",
       "| 5 | VLOG103 |  66 |\n",
       "| 6 | VLOG104 |  79 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  total_adjective\n",
       "1 VLOG1    44            \n",
       "2 VLOG10   50            \n",
       "3 VLOG100  21            \n",
       "4 VLOG102 106            \n",
       "5 VLOG103  66            \n",
       "6 VLOG104  79            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adjective_data = read.table(\"../input/adjectives-data/adjective_list.txt\", header = TRUE, sep = '')\n",
    "\n",
    "\n",
    "adjective_df <- inner_join(transcripts_unnest, adjective_data, by = 'word')  \n",
    "\n",
    "\n",
    "# Grouping adjectives use by vlogId\n",
    "adjective_df <- adjective_df %>% \n",
    "group_by(vlogId) %>%\n",
    "count(word)\n",
    "\n",
    "# summing total number of adjectives used per vlogger \n",
    "adjectives <- \n",
    "    adjective_df[, 3] %>%\n",
    "    aggregate(list(adjective_df$vlogId), sum) %>% # changing the column names so we can join them to another table by vlogId later \n",
    "  rename(total_adjective = n, vlogId = Group.1)\n",
    "\n",
    "head(adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6a751",
   "metadata": {
    "papermill": {
     "duration": 0.043491,
     "end_time": "2021-09-21T11:02:38.382942",
     "exception": false,
     "start_time": "2021-09-21T11:02:38.339451",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fca10fb",
   "metadata": {
    "papermill": {
     "duration": 0.043742,
     "end_time": "2021-09-21T11:02:38.470361",
     "exception": false,
     "start_time": "2021-09-21T11:02:38.426619",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5fc8bbb",
   "metadata": {
    "papermill": {
     "duration": 0.043678,
     "end_time": "2021-09-21T11:02:38.557991",
     "exception": false,
     "start_time": "2021-09-21T11:02:38.514313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature 6: Affin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c55ee10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:38.651201Z",
     "iopub.status.busy": "2021-09-21T11:02:38.649540Z",
     "iopub.status.idle": "2021-09-21T11:02:39.580190Z",
     "shell.execute_reply": "2021-09-21T11:02:39.579053Z"
    },
    "papermill": {
     "duration": 0.978681,
     "end_time": "2021-09-21T11:02:39.580332",
     "exception": false,
     "start_time": "2021-09-21T11:02:38.601651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>affin</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG1  </td><td>40</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG10 </td><td>29</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG100</td><td>21</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG102</td><td>68</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG103</td><td>38</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG104</td><td>28</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & vlogId & affin\\\\\n",
       "  & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG1   & 40\\\\\n",
       "\t2 & VLOG10  & 29\\\\\n",
       "\t3 & VLOG100 & 21\\\\\n",
       "\t4 & VLOG102 & 68\\\\\n",
       "\t5 & VLOG103 & 38\\\\\n",
       "\t6 & VLOG104 & 28\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | affin &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| 1 | VLOG1   | 40 |\n",
       "| 2 | VLOG10  | 29 |\n",
       "| 3 | VLOG100 | 21 |\n",
       "| 4 | VLOG102 | 68 |\n",
       "| 5 | VLOG103 | 38 |\n",
       "| 6 | VLOG104 | 28 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  affin\n",
       "1 VLOG1   40   \n",
       "2 VLOG10  29   \n",
       "3 VLOG100 21   \n",
       "4 VLOG102 68   \n",
       "5 VLOG103 38   \n",
       "6 VLOG104 28   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# FEATURE EXTRACTION: Afinn \n",
    "\n",
    "# Import AFINN\n",
    "\n",
    "download.file(\"http://www2.imm.dtu.dk/pubdb/edoc/imm6010.zip\",\"afinn.zip\")\n",
    "unzip(\"afinn.zip\")\n",
    "afinn <- read.delim(\"AFINN/AFINN-111.txt\", sep=\"\\t\", col.names = c(\"word\",\"score\"), stringsAsFactors = FALSE)\n",
    "\n",
    "# Do an inner join an essay token data fame and afinn word list\n",
    "\n",
    "transcripts_affin <- \n",
    "    inner_join(transcripts_unnest, afinn, by = 'word') \n",
    "\n",
    "# grouping affin score by vlogId\n",
    "transcripts_affin <- transcripts_affin %>% \n",
    "group_by(vlogId) %>%\n",
    "count(word)\n",
    "\n",
    "# summing total total affin score per vlogger \n",
    "affin <- \n",
    "    transcripts_affin$n %>%\n",
    "    aggregate(list(transcripts_affin$vlogId), sum) %>% # changing the column names so we can join them to another table by vlogId later \n",
    "  rename(affin = x, vlogId = Group.1)\n",
    "\n",
    "# unlist to make data appropriate for the regression analysis \n",
    "affin <- affin %>% mutate(affin = unlist(affin)) \n",
    "head(affin) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee051b8",
   "metadata": {
    "papermill": {
     "duration": 0.045118,
     "end_time": "2021-09-21T11:02:39.670863",
     "exception": false,
     "start_time": "2021-09-21T11:02:39.625745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature 7: NCR lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c13e9c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:39.766488Z",
     "iopub.status.busy": "2021-09-21T11:02:39.765103Z",
     "iopub.status.idle": "2021-09-21T11:02:41.614739Z",
     "shell.execute_reply": "2021-09-21T11:02:41.613006Z"
    },
    "papermill": {
     "duration": 1.899066,
     "end_time": "2021-09-21T11:02:41.615014",
     "exception": false,
     "start_time": "2021-09-21T11:02:39.715948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>word</th><th scope=col>sentiment</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>radiant  </td><td>joy     </td></tr>\n",
       "\t<tr><td>treason  </td><td>fear    </td></tr>\n",
       "\t<tr><td>publicist</td><td>negative</td></tr>\n",
       "\t<tr><td>assignee </td><td>trust   </td></tr>\n",
       "\t<tr><td>ineffable</td><td>positive</td></tr>\n",
       "\t<tr><td>gorgeous </td><td>positive</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " word & sentiment\\\\\n",
       " <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t radiant   & joy     \\\\\n",
       "\t treason   & fear    \\\\\n",
       "\t publicist & negative\\\\\n",
       "\t assignee  & trust   \\\\\n",
       "\t ineffable & positive\\\\\n",
       "\t gorgeous  & positive\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| word &lt;chr&gt; | sentiment &lt;chr&gt; |\n",
       "|---|---|\n",
       "| radiant   | joy      |\n",
       "| treason   | fear     |\n",
       "| publicist | negative |\n",
       "| assignee  | trust    |\n",
       "| ineffable | positive |\n",
       "| gorgeous  | positive |\n",
       "\n"
      ],
      "text/plain": [
       "  word      sentiment\n",
       "1 radiant   joy      \n",
       "2 treason   fear     \n",
       "3 publicist negative \n",
       "4 assignee  trust    \n",
       "5 ineffable positive \n",
       "6 gorgeous  positive "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>word</th><th scope=col>sentiment</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>insult</td><td>anger   </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>insult</td><td>disgust </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>insult</td><td>negative</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>insult</td><td>sadness </td></tr>\n",
       "\t<tr><td>VLOG1</td><td>insult</td><td>surprise</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>trade </td><td>trust   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " vlogId & word & sentiment\\\\\n",
       " <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & insult & anger   \\\\\n",
       "\t VLOG1 & insult & disgust \\\\\n",
       "\t VLOG1 & insult & negative\\\\\n",
       "\t VLOG1 & insult & sadness \\\\\n",
       "\t VLOG1 & insult & surprise\\\\\n",
       "\t VLOG1 & trade  & trust   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 3\n",
       "\n",
       "| vlogId &lt;chr&gt; | word &lt;chr&gt; | sentiment &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| VLOG1 | insult | anger    |\n",
       "| VLOG1 | insult | disgust  |\n",
       "| VLOG1 | insult | negative |\n",
       "| VLOG1 | insult | sadness  |\n",
       "| VLOG1 | insult | surprise |\n",
       "| VLOG1 | trade  | trust    |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId word   sentiment\n",
       "1 VLOG1  insult anger    \n",
       "2 VLOG1  insult disgust  \n",
       "3 VLOG1  insult negative \n",
       "4 VLOG1  insult sadness  \n",
       "5 VLOG1  insult surprise \n",
       "6 VLOG1  trade  trust    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>40649</li><li>3</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 40649\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 40649\n",
       "2. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 40649     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3782</li><li>3</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3782\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3782\n",
       "2. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3782    3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>sentiment</th><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1</td><td>anger       </td><td> 8</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>anticipation</td><td>10</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>disgust     </td><td> 8</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>fear        </td><td> 6</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>joy         </td><td> 7</td></tr>\n",
       "\t<tr><td>VLOG1</td><td>negative    </td><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " vlogId & sentiment & n\\\\\n",
       " <chr> & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t VLOG1 & anger        &  8\\\\\n",
       "\t VLOG1 & anticipation & 10\\\\\n",
       "\t VLOG1 & disgust      &  8\\\\\n",
       "\t VLOG1 & fear         &  6\\\\\n",
       "\t VLOG1 & joy          &  7\\\\\n",
       "\t VLOG1 & negative     & 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 3\n",
       "\n",
       "| vlogId &lt;chr&gt; | sentiment &lt;chr&gt; | n &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| VLOG1 | anger        |  8 |\n",
       "| VLOG1 | anticipation | 10 |\n",
       "| VLOG1 | disgust      |  8 |\n",
       "| VLOG1 | fear         |  6 |\n",
       "| VLOG1 | joy          |  7 |\n",
       "| VLOG1 | negative     | 10 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId sentiment    n \n",
       "1 VLOG1  anger         8\n",
       "2 VLOG1  anticipation 10\n",
       "3 VLOG1  disgust       8\n",
       "4 VLOG1  fear          6\n",
       "5 VLOG1  joy           7\n",
       "6 VLOG1  negative     10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 11</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>anger</th><th scope=col>anticipation</th><th scope=col>disgust</th><th scope=col>fear</th><th scope=col>joy</th><th scope=col>negative</th><th scope=col>positive</th><th scope=col>sadness</th><th scope=col>surprise</th><th scope=col>trust</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG1  </td><td> 8</td><td>10</td><td>8</td><td> 6</td><td> 7</td><td>10</td><td>14</td><td> 8</td><td> 5</td><td>11</td></tr>\n",
       "\t<tr><td>VLOG10 </td><td>10</td><td>19</td><td>7</td><td>14</td><td> 9</td><td>18</td><td>19</td><td> 8</td><td>10</td><td>17</td></tr>\n",
       "\t<tr><td>VLOG100</td><td> 0</td><td> 9</td><td>0</td><td> 2</td><td> 8</td><td> 3</td><td>11</td><td> 1</td><td> 2</td><td> 9</td></tr>\n",
       "\t<tr><td>VLOG102</td><td> 4</td><td>50</td><td>7</td><td>14</td><td>29</td><td>14</td><td>42</td><td> 8</td><td>22</td><td>24</td></tr>\n",
       "\t<tr><td>VLOG103</td><td> 0</td><td>19</td><td>4</td><td> 9</td><td>13</td><td>15</td><td>26</td><td>10</td><td> 5</td><td>11</td></tr>\n",
       "\t<tr><td>VLOG104</td><td> 2</td><td>15</td><td>0</td><td> 4</td><td>49</td><td> 3</td><td>62</td><td>22</td><td> 9</td><td>37</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 11\n",
       "\\begin{tabular}{lllllllllll}\n",
       " vlogId & anger & anticipation & disgust & fear & joy & negative & positive & sadness & surprise & trust\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG1   &  8 & 10 & 8 &  6 &  7 & 10 & 14 &  8 &  5 & 11\\\\\n",
       "\t VLOG10  & 10 & 19 & 7 & 14 &  9 & 18 & 19 &  8 & 10 & 17\\\\\n",
       "\t VLOG100 &  0 &  9 & 0 &  2 &  8 &  3 & 11 &  1 &  2 &  9\\\\\n",
       "\t VLOG102 &  4 & 50 & 7 & 14 & 29 & 14 & 42 &  8 & 22 & 24\\\\\n",
       "\t VLOG103 &  0 & 19 & 4 &  9 & 13 & 15 & 26 & 10 &  5 & 11\\\\\n",
       "\t VLOG104 &  2 & 15 & 0 &  4 & 49 &  3 & 62 & 22 &  9 & 37\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 11\n",
       "\n",
       "| vlogId &lt;chr&gt; | anger &lt;dbl&gt; | anticipation &lt;dbl&gt; | disgust &lt;dbl&gt; | fear &lt;dbl&gt; | joy &lt;dbl&gt; | negative &lt;dbl&gt; | positive &lt;dbl&gt; | sadness &lt;dbl&gt; | surprise &lt;dbl&gt; | trust &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| VLOG1   |  8 | 10 | 8 |  6 |  7 | 10 | 14 |  8 |  5 | 11 |\n",
       "| VLOG10  | 10 | 19 | 7 | 14 |  9 | 18 | 19 |  8 | 10 | 17 |\n",
       "| VLOG100 |  0 |  9 | 0 |  2 |  8 |  3 | 11 |  1 |  2 |  9 |\n",
       "| VLOG102 |  4 | 50 | 7 | 14 | 29 | 14 | 42 |  8 | 22 | 24 |\n",
       "| VLOG103 |  0 | 19 | 4 |  9 | 13 | 15 | 26 | 10 |  5 | 11 |\n",
       "| VLOG104 |  2 | 15 | 0 |  4 | 49 |  3 | 62 | 22 |  9 | 37 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId  anger anticipation disgust fear joy negative positive sadness\n",
       "1 VLOG1    8    10           8        6    7  10       14        8     \n",
       "2 VLOG10  10    19           7       14    9  18       19        8     \n",
       "3 VLOG100  0     9           0        2    8   3       11        1     \n",
       "4 VLOG102  4    50           7       14   29  14       42        8     \n",
       "5 VLOG103  0    19           4        9   13  15       26       10     \n",
       "6 VLOG104  2    15           0        4   49   3       62       22     \n",
       "  surprise trust\n",
       "1  5       11   \n",
       "2 10       17   \n",
       "3  2        9   \n",
       "4 22       24   \n",
       "5  5       11   \n",
       "6  9       37   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####NRC OUTPUT\n",
    "\n",
    "#Feature extraction load NRC word list\n",
    "load_nrc = function() {\n",
    "    if (!file.exists('nrc.txt'))\n",
    "        download.file(\"https://www.dropbox.com/s/yo5o476zk8j5ujg/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt?dl=1\",\"nrc.txt\")\n",
    "    nrc = read.table(\"nrc.txt\", col.names=c('word','sentiment','applies'), stringsAsFactors = FALSE)\n",
    "    nrc %>% filter(applies==1) %>% select(-applies)\n",
    "}\n",
    "\n",
    "nrc = load_nrc()\n",
    "sample_n(nrc, 6)\n",
    "\n",
    "nrc = load_nrc()\n",
    "\n",
    "# Do an inner join an essay token data fame and nrc word list\n",
    "transcript_ncr = \n",
    "    inner_join(transcripts_unnest, nrc, by = c(word = 'word')) \n",
    "\n",
    "# Peek at the result\n",
    "head(transcript_ncr, 6)\n",
    "dim(transcript_ncr)\n",
    "\n",
    "#count the sentiment\n",
    "transcript_sentiment_scores = \n",
    "    transcript_ncr %>%\n",
    "    count(`vlogId`, sentiment) \n",
    "\n",
    "# Peek at the result\n",
    "dim(transcript_sentiment_scores)\n",
    "head(transcript_sentiment_scores)\n",
    "\n",
    "\n",
    "#widen the table\n",
    "sentiment = \n",
    "    transcript_sentiment_scores %>%\n",
    "    spread(sentiment, n, fill = 0)\n",
    "\n",
    "head(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e6a77",
   "metadata": {
    "papermill": {
     "duration": 0.049632,
     "end_time": "2021-09-21T11:02:41.714612",
     "exception": false,
     "start_time": "2021-09-21T11:02:41.664980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Combining our features into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd75fac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:41.823552Z",
     "iopub.status.busy": "2021-09-21T11:02:41.821139Z",
     "iopub.status.idle": "2021-09-21T11:02:41.884175Z",
     "shell.execute_reply": "2021-09-21T11:02:41.882211Z"
    },
    "papermill": {
     "duration": 0.119857,
     "end_time": "2021-09-21T11:02:41.884352",
     "exception": false,
     "start_time": "2021-09-21T11:02:41.764495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 23</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>gender</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th><th scope=col>n_words</th><th scope=col>word_length</th><th scope=col>empathy</th><th scope=col>⋯</th><th scope=col>anger</th><th scope=col>anticipation</th><th scope=col>disgust</th><th scope=col>fear</th><th scope=col>joy</th><th scope=col>negative</th><th scope=col>positive</th><th scope=col>sadness</th><th scope=col>surprise</th><th scope=col>trust</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG3</td><td>Female</td><td>5.0</td><td>5.0</td><td>4.6</td><td>5.3</td><td>4.4</td><td>375</td><td>4.130667</td><td>3.222870</td><td>⋯</td><td> 1</td><td>11</td><td> 2</td><td>2</td><td> 7</td><td> 2</td><td>10</td><td> 4</td><td> 6</td><td> 9</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG5</td><td>Male  </td><td>5.9</td><td>5.3</td><td>5.3</td><td>5.8</td><td>5.5</td><td>395</td><td>3.944304</td><td>3.174061</td><td>⋯</td><td> 1</td><td> 6</td><td> 1</td><td>2</td><td> 4</td><td> 1</td><td>10</td><td> 1</td><td> 4</td><td> 5</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG6</td><td>Male  </td><td>5.4</td><td>4.8</td><td>4.4</td><td>4.8</td><td>5.7</td><td>622</td><td>4.146302</td><td>3.116102</td><td>⋯</td><td> 5</td><td>11</td><td> 4</td><td>4</td><td>19</td><td>12</td><td>29</td><td> 3</td><td>11</td><td>24</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG7</td><td>Male  </td><td>4.7</td><td>5.1</td><td>4.4</td><td>5.1</td><td>4.7</td><td>644</td><td>4.099379</td><td>3.134713</td><td>⋯</td><td>12</td><td>22</td><td>11</td><td>7</td><td>12</td><td>15</td><td>21</td><td>11</td><td> 9</td><td>16</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG8</td><td>Female</td><td> NA</td><td> NA</td><td> NA</td><td> NA</td><td> NA</td><td>311</td><td>3.729904</td><td>3.159109</td><td>⋯</td><td> 3</td><td>12</td><td> 2</td><td>6</td><td> 7</td><td> 5</td><td>12</td><td> 4</td><td> 3</td><td> 9</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG9</td><td>Female</td><td>5.6</td><td>5.0</td><td>4.0</td><td>4.2</td><td>4.9</td><td>873</td><td>4.019473</td><td>3.169586</td><td>⋯</td><td> 9</td><td>17</td><td> 8</td><td>8</td><td>15</td><td>18</td><td>30</td><td> 5</td><td>11</td><td>21</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 23\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & vlogId & gender & Extr & Agr & Cons & Emot & Open & n\\_words & word\\_length & empathy & ⋯ & anger & anticipation & disgust & fear & joy & negative & positive & sadness & surprise & trust\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG3 & Female & 5.0 & 5.0 & 4.6 & 5.3 & 4.4 & 375 & 4.130667 & 3.222870 & ⋯ &  1 & 11 &  2 & 2 &  7 &  2 & 10 &  4 &  6 &  9\\\\\n",
       "\t2 & VLOG5 & Male   & 5.9 & 5.3 & 5.3 & 5.8 & 5.5 & 395 & 3.944304 & 3.174061 & ⋯ &  1 &  6 &  1 & 2 &  4 &  1 & 10 &  1 &  4 &  5\\\\\n",
       "\t3 & VLOG6 & Male   & 5.4 & 4.8 & 4.4 & 4.8 & 5.7 & 622 & 4.146302 & 3.116102 & ⋯ &  5 & 11 &  4 & 4 & 19 & 12 & 29 &  3 & 11 & 24\\\\\n",
       "\t4 & VLOG7 & Male   & 4.7 & 5.1 & 4.4 & 5.1 & 4.7 & 644 & 4.099379 & 3.134713 & ⋯ & 12 & 22 & 11 & 7 & 12 & 15 & 21 & 11 &  9 & 16\\\\\n",
       "\t5 & VLOG8 & Female &  NA &  NA &  NA &  NA &  NA & 311 & 3.729904 & 3.159109 & ⋯ &  3 & 12 &  2 & 6 &  7 &  5 & 12 &  4 &  3 &  9\\\\\n",
       "\t6 & VLOG9 & Female & 5.6 & 5.0 & 4.0 & 4.2 & 4.9 & 873 & 4.019473 & 3.169586 & ⋯ &  9 & 17 &  8 & 8 & 15 & 18 & 30 &  5 & 11 & 21\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 23\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | gender &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; | n_words &lt;int&gt; | word_length &lt;dbl&gt; | empathy &lt;dbl&gt; | ⋯ ⋯ | anger &lt;dbl&gt; | anticipation &lt;dbl&gt; | disgust &lt;dbl&gt; | fear &lt;dbl&gt; | joy &lt;dbl&gt; | negative &lt;dbl&gt; | positive &lt;dbl&gt; | sadness &lt;dbl&gt; | surprise &lt;dbl&gt; | trust &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | VLOG3 | Female | 5.0 | 5.0 | 4.6 | 5.3 | 4.4 | 375 | 4.130667 | 3.222870 | ⋯ |  1 | 11 |  2 | 2 |  7 |  2 | 10 |  4 |  6 |  9 |\n",
       "| 2 | VLOG5 | Male   | 5.9 | 5.3 | 5.3 | 5.8 | 5.5 | 395 | 3.944304 | 3.174061 | ⋯ |  1 |  6 |  1 | 2 |  4 |  1 | 10 |  1 |  4 |  5 |\n",
       "| 3 | VLOG6 | Male   | 5.4 | 4.8 | 4.4 | 4.8 | 5.7 | 622 | 4.146302 | 3.116102 | ⋯ |  5 | 11 |  4 | 4 | 19 | 12 | 29 |  3 | 11 | 24 |\n",
       "| 4 | VLOG7 | Male   | 4.7 | 5.1 | 4.4 | 5.1 | 4.7 | 644 | 4.099379 | 3.134713 | ⋯ | 12 | 22 | 11 | 7 | 12 | 15 | 21 | 11 |  9 | 16 |\n",
       "| 5 | VLOG8 | Female |  NA |  NA |  NA |  NA |  NA | 311 | 3.729904 | 3.159109 | ⋯ |  3 | 12 |  2 | 6 |  7 |  5 | 12 |  4 |  3 |  9 |\n",
       "| 6 | VLOG9 | Female | 5.6 | 5.0 | 4.0 | 4.2 | 4.9 | 873 | 4.019473 | 3.169586 | ⋯ |  9 | 17 |  8 | 8 | 15 | 18 | 30 |  5 | 11 | 21 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId gender Extr Agr Cons Emot Open n_words word_length empathy  ⋯ anger\n",
       "1 VLOG3  Female 5.0  5.0 4.6  5.3  4.4  375     4.130667    3.222870 ⋯  1   \n",
       "2 VLOG5  Male   5.9  5.3 5.3  5.8  5.5  395     3.944304    3.174061 ⋯  1   \n",
       "3 VLOG6  Male   5.4  4.8 4.4  4.8  5.7  622     4.146302    3.116102 ⋯  5   \n",
       "4 VLOG7  Male   4.7  5.1 4.4  5.1  4.7  644     4.099379    3.134713 ⋯ 12   \n",
       "5 VLOG8  Female  NA   NA  NA   NA   NA  311     3.729904    3.159109 ⋯  3   \n",
       "6 VLOG9  Female 5.6  5.0 4.0  4.2  4.9  873     4.019473    3.169586 ⋯  9   \n",
       "  anticipation disgust fear joy negative positive sadness surprise trust\n",
       "1 11            2      2     7   2       10        4       6        9   \n",
       "2  6            1      2     4   1       10        1       4        5   \n",
       "3 11            4      4    19  12       29        3      11       24   \n",
       "4 22           11      7    12  15       21       11       9       16   \n",
       "5 12            2      6     7   5       12        4       3        9   \n",
       "6 17            8      8    15  18       30        5      11       21   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combining all features to one table \n",
    "transcript_features_df <- vlogger_df %>%\n",
    "    inner_join(word_count, by = \"vlogId\") %>%\n",
    "    inner_join(word_length, by = \"vlogId\") %>%\n",
    "    inner_join(empathy, by = \"vlogId\") %>%\n",
    "    inner_join(distress, by = \"vlogId\") %>%\n",
    "    inner_join(adjectives, by = \"vlogId\") %>%\n",
    "    inner_join(affin, by = \"vlogId\") %>%\n",
    "    inner_join(sentiment, by = \"vlogId\")\n",
    "\n",
    "head(transcript_features_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bb925",
   "metadata": {
    "_cell_guid": "ba49ed13-9045-4b21-9f2e-09b58bc02b41",
    "_uuid": "54aafd4b-9834-4086-8030-702069f39518",
    "papermill": {
     "duration": 0.053287,
     "end_time": "2021-09-21T11:02:41.990272",
     "exception": false,
     "start_time": "2021-09-21T11:02:41.936985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Predictive model\n",
    "We chose a multiple linear regression anaylsis because we want to predict a continuous outcome variable. First we tried the full model with all of our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd346dec",
   "metadata": {
    "_cell_guid": "8a713b08-df61-4c46-b392-1084c9f357b4",
    "_uuid": "972d1589-1ba3-49b3-bae8-9e09c7666a21",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:42.097667Z",
     "iopub.status.busy": "2021-09-21T11:02:42.095396Z",
     "iopub.status.idle": "2021-09-21T11:02:42.210404Z",
     "shell.execute_reply": "2021-09-21T11:02:42.207977Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.169901,
     "end_time": "2021-09-21T11:02:42.210622",
     "exception": false,
     "start_time": "2021-09-21T11:02:42.040721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 6 × 23 of type lgl</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>vlogId</th><th scope=col>gender</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th><th scope=col>n_words</th><th scope=col>word_length</th><th scope=col>empathy</th><th scope=col>⋯</th><th scope=col>anger</th><th scope=col>anticipation</th><th scope=col>disgust</th><th scope=col>fear</th><th scope=col>joy</th><th scope=col>negative</th><th scope=col>positive</th><th scope=col>sadness</th><th scope=col>surprise</th><th scope=col>trust</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>FALSE</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td></tr>\n",
       "\t<tr><td>FALSE</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td></tr>\n",
       "\t<tr><td>FALSE</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td></tr>\n",
       "\t<tr><td>FALSE</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td></tr>\n",
       "\t<tr><td>FALSE</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td></tr>\n",
       "\t<tr><td>FALSE</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>⋯</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>FALSE</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6 × 23 of type lgl\n",
       "\\begin{tabular}{lllllllllllllllllllll}\n",
       " vlogId & gender & Extr & Agr & Cons & Emot & Open & n\\_words & word\\_length & empathy & ⋯ & anger & anticipation & disgust & fear & joy & negative & positive & sadness & surprise & trust\\\\\n",
       "\\hline\n",
       "\t FALSE & FALSE & TRUE & TRUE & TRUE & TRUE & TRUE & FALSE & FALSE & FALSE & ⋯ & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE\\\\\n",
       "\t FALSE & FALSE & TRUE & TRUE & TRUE & TRUE & TRUE & FALSE & FALSE & FALSE & ⋯ & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE\\\\\n",
       "\t FALSE & FALSE & TRUE & TRUE & TRUE & TRUE & TRUE & FALSE & FALSE & FALSE & ⋯ & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE\\\\\n",
       "\t FALSE & FALSE & TRUE & TRUE & TRUE & TRUE & TRUE & FALSE & FALSE & FALSE & ⋯ & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE\\\\\n",
       "\t FALSE & FALSE & TRUE & TRUE & TRUE & TRUE & TRUE & FALSE & FALSE & FALSE & ⋯ & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE\\\\\n",
       "\t FALSE & FALSE & TRUE & TRUE & TRUE & TRUE & TRUE & FALSE & FALSE & FALSE & ⋯ & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE & FALSE\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6 × 23 of type lgl\n",
       "\n",
       "| vlogId | gender | Extr | Agr | Cons | Emot | Open | n_words | word_length | empathy | ⋯ | anger | anticipation | disgust | fear | joy | negative | positive | sadness | surprise | trust |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| FALSE | FALSE | TRUE | TRUE | TRUE | TRUE | TRUE | FALSE | FALSE | FALSE | ⋯ | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE |\n",
       "| FALSE | FALSE | TRUE | TRUE | TRUE | TRUE | TRUE | FALSE | FALSE | FALSE | ⋯ | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE |\n",
       "| FALSE | FALSE | TRUE | TRUE | TRUE | TRUE | TRUE | FALSE | FALSE | FALSE | ⋯ | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE |\n",
       "| FALSE | FALSE | TRUE | TRUE | TRUE | TRUE | TRUE | FALSE | FALSE | FALSE | ⋯ | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE |\n",
       "| FALSE | FALSE | TRUE | TRUE | TRUE | TRUE | TRUE | FALSE | FALSE | FALSE | ⋯ | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE |\n",
       "| FALSE | FALSE | TRUE | TRUE | TRUE | TRUE | TRUE | FALSE | FALSE | FALSE | ⋯ | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE | FALSE |\n",
       "\n"
      ],
      "text/plain": [
       "     vlogId gender Extr Agr  Cons Emot Open n_words word_length empathy ⋯ anger\n",
       "[1,] FALSE  FALSE  TRUE TRUE TRUE TRUE TRUE FALSE   FALSE       FALSE   ⋯ FALSE\n",
       "[2,] FALSE  FALSE  TRUE TRUE TRUE TRUE TRUE FALSE   FALSE       FALSE   ⋯ FALSE\n",
       "[3,] FALSE  FALSE  TRUE TRUE TRUE TRUE TRUE FALSE   FALSE       FALSE   ⋯ FALSE\n",
       "[4,] FALSE  FALSE  TRUE TRUE TRUE TRUE TRUE FALSE   FALSE       FALSE   ⋯ FALSE\n",
       "[5,] FALSE  FALSE  TRUE TRUE TRUE TRUE TRUE FALSE   FALSE       FALSE   ⋯ FALSE\n",
       "[6,] FALSE  FALSE  TRUE TRUE TRUE TRUE TRUE FALSE   FALSE       FALSE   ⋯ FALSE\n",
       "     anticipation disgust fear  joy   negative positive sadness surprise trust\n",
       "[1,] FALSE        FALSE   FALSE FALSE FALSE    FALSE    FALSE   FALSE    FALSE\n",
       "[2,] FALSE        FALSE   FALSE FALSE FALSE    FALSE    FALSE   FALSE    FALSE\n",
       "[3,] FALSE        FALSE   FALSE FALSE FALSE    FALSE    FALSE   FALSE    FALSE\n",
       "[4,] FALSE        FALSE   FALSE FALSE FALSE    FALSE    FALSE   FALSE    FALSE\n",
       "[5,] FALSE        FALSE   FALSE FALSE FALSE    FALSE    FALSE   FALSE    FALSE\n",
       "[6,] FALSE        FALSE   FALSE FALSE FALSE    FALSE    FALSE   FALSE    FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Response Extr :\n",
       "\n",
       "Call:\n",
       "lm(formula = Extr ~ n_words + word_length + empathy + distress + \n",
       "    total_adjective + affin + anger + anticipation + fear + joy + \n",
       "    negative + positive + sadness + surprise + disgust + trust, \n",
       "    data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.58951 -0.70097  0.07169  0.70424  2.10534 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)   \n",
       "(Intercept)     -1.9225791  2.3245612  -0.827  0.40884   \n",
       "n_words         -0.0002287  0.0005854  -0.391  0.69636   \n",
       "word_length      0.3385940  0.3215940   1.053  0.29324   \n",
       "empathy         -1.1707475  0.9854369  -1.188  0.23574   \n",
       "distress         2.9412012  0.9630783   3.054  0.00246 **\n",
       "total_adjective  0.0006348  0.0052245   0.121  0.90338   \n",
       "affin            0.0068707  0.0045477   1.511  0.13187   \n",
       "anger            0.0213527  0.0218924   0.975  0.33016   \n",
       "anticipation    -0.0102445  0.0129634  -0.790  0.42999   \n",
       "fear            -0.0137038  0.0167566  -0.818  0.41410   \n",
       "joy              0.0209539  0.0168570   1.243  0.21480   \n",
       "negative        -0.0064410  0.0164698  -0.391  0.69601   \n",
       "positive         0.0039900  0.0116367   0.343  0.73192   \n",
       "sadness         -0.0158463  0.0192845  -0.822  0.41188   \n",
       "surprise         0.0206518  0.0185838   1.111  0.26732   \n",
       "disgust          0.0083123  0.0214409   0.388  0.69852   \n",
       "trust           -0.0179595  0.0139137  -1.291  0.19776   \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.9522 on 306 degrees of freedom\n",
       "Multiple R-squared:  0.07942,\tAdjusted R-squared:  0.03129 \n",
       "F-statistic:  1.65 on 16 and 306 DF,  p-value: 0.05553\n",
       "\n",
       "\n",
       "Response Agr :\n",
       "\n",
       "Call:\n",
       "lm(formula = Agr ~ n_words + word_length + empathy + distress + \n",
       "    total_adjective + affin + anger + anticipation + fear + joy + \n",
       "    negative + positive + sadness + surprise + disgust + trust, \n",
       "    data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.57588 -0.43019  0.06978  0.50258  1.89463 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)      0.6626690  1.8224293   0.364 0.716394    \n",
       "n_words          0.0002192  0.0004590   0.478 0.633209    \n",
       "word_length     -0.6347987  0.2521260  -2.518 0.012320 *  \n",
       "empathy          4.7765359  0.7725712   6.183 2.01e-09 ***\n",
       "distress        -2.8031574  0.7550424  -3.713 0.000244 ***\n",
       "total_adjective  0.0033903  0.0040960   0.828 0.408482    \n",
       "affin           -0.0061646  0.0035653  -1.729 0.084812 .  \n",
       "anger           -0.0433863  0.0171634  -2.528 0.011980 *  \n",
       "anticipation    -0.0060709  0.0101632  -0.597 0.550718    \n",
       "fear             0.0387836  0.0131370   2.952 0.003399 ** \n",
       "joy              0.0231028  0.0132157   1.748 0.081443 .  \n",
       "negative         0.0024718  0.0129121   0.191 0.848315    \n",
       "positive        -0.0033328  0.0091230  -0.365 0.715129    \n",
       "sadness         -0.0356588  0.0151188  -2.359 0.018974 *  \n",
       "surprise         0.0289810  0.0145695   1.989 0.047575 *  \n",
       "disgust         -0.0349649  0.0168094  -2.080 0.038350 *  \n",
       "trust           -0.0133613  0.0109082  -1.225 0.221561    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7465 on 306 degrees of freedom\n",
       "Multiple R-squared:  0.3331,\tAdjusted R-squared:  0.2982 \n",
       "F-statistic: 9.551 on 16 and 306 DF,  p-value: < 2.2e-16\n",
       "\n",
       "\n",
       "Response Cons :\n",
       "\n",
       "Call:\n",
       "lm(formula = Cons ~ n_words + word_length + empathy + distress + \n",
       "    total_adjective + affin + anger + anticipation + fear + joy + \n",
       "    negative + positive + sadness + surprise + disgust + trust, \n",
       "    data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.02395 -0.37268  0.01819  0.43728  1.89138 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)  \n",
       "(Intercept)     -1.4917918  1.7731929  -0.841   0.4008  \n",
       "n_words          0.0002515  0.0004466   0.563   0.5737  \n",
       "word_length      0.6335606  0.2453144   2.583   0.0103 *\n",
       "empathy          1.8532641  0.7516987   2.465   0.0142 *\n",
       "distress        -0.8051400  0.7346435  -1.096   0.2740  \n",
       "total_adjective  0.0018849  0.0039853   0.473   0.6366  \n",
       "affin           -0.0077150  0.0034690  -2.224   0.0269 *\n",
       "anger           -0.0291791  0.0166997  -1.747   0.0816 .\n",
       "anticipation    -0.0125341  0.0098886  -1.268   0.2059  \n",
       "fear             0.0297875  0.0127820   2.330   0.0204 *\n",
       "joy             -0.0078407  0.0128586  -0.610   0.5425  \n",
       "negative        -0.0033727  0.0125633  -0.268   0.7885  \n",
       "positive         0.0216526  0.0088765   2.439   0.0153 *\n",
       "sadness          0.0030081  0.0147103   0.204   0.8381  \n",
       "surprise         0.0202775  0.0141759   1.430   0.1536  \n",
       "disgust         -0.0232540  0.0163552  -1.422   0.1561  \n",
       "trust           -0.0096203  0.0106135  -0.906   0.3654  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7263 on 306 degrees of freedom\n",
       "Multiple R-squared:  0.1958,\tAdjusted R-squared:  0.1537 \n",
       "F-statistic: 4.656 on 16 and 306 DF,  p-value: 2.089e-08\n",
       "\n",
       "\n",
       "Response Emot :\n",
       "\n",
       "Call:\n",
       "lm(formula = Emot ~ n_words + word_length + empathy + distress + \n",
       "    total_adjective + affin + anger + anticipation + fear + joy + \n",
       "    negative + positive + sadness + surprise + disgust + trust, \n",
       "    data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.22057 -0.39069  0.07256  0.51415  1.92473 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)      4.1908506  1.7493132   2.396 0.017189 *  \n",
       "n_words         -0.0001090  0.0004406  -0.247 0.804761    \n",
       "word_length     -0.1486815  0.2420107  -0.614 0.539435    \n",
       "empathy          2.7023743  0.7415756   3.644 0.000315 ***\n",
       "distress        -2.4537362  0.7247500  -3.386 0.000803 ***\n",
       "total_adjective  0.0049866  0.0039316   1.268 0.205650    \n",
       "affin           -0.0074330  0.0034223  -2.172 0.030628 *  \n",
       "anger           -0.0236475  0.0164748  -1.435 0.152202    \n",
       "anticipation    -0.0114122  0.0097554  -1.170 0.242980    \n",
       "fear             0.0269899  0.0126099   2.140 0.033115 *  \n",
       "joy              0.0070717  0.0126855   0.557 0.577618    \n",
       "negative         0.0022423  0.0123941   0.181 0.856550    \n",
       "positive         0.0111991  0.0087570   1.279 0.201911    \n",
       "sadness         -0.0335307  0.0145122  -2.311 0.021525 *  \n",
       "surprise         0.0227109  0.0139850   1.624 0.105416    \n",
       "disgust         -0.0161440  0.0161350  -1.001 0.317831    \n",
       "trust           -0.0052344  0.0104706  -0.500 0.617493    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7165 on 306 degrees of freedom\n",
       "Multiple R-squared:  0.1803,\tAdjusted R-squared:  0.1374 \n",
       "F-statistic: 4.207 on 16 and 306 DF,  p-value: 2.219e-07\n",
       "\n",
       "\n",
       "Response Open :\n",
       "\n",
       "Call:\n",
       "lm(formula = Open ~ n_words + word_length + empathy + distress + \n",
       "    total_adjective + affin + anger + anticipation + fear + joy + \n",
       "    negative + positive + sadness + surprise + disgust + trust, \n",
       "    data = training_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-2.3101 -0.4884  0.0028  0.4348  1.5586 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)\n",
       "(Intercept)      1.7581701  1.7157636   1.025    0.306\n",
       "n_words         -0.0001542  0.0004321  -0.357    0.721\n",
       "word_length     -0.1476605  0.2373693  -0.622    0.534\n",
       "empathy          0.4967889  0.7273531   0.683    0.495\n",
       "distress         0.6581120  0.7108502   0.926    0.355\n",
       "total_adjective -0.0009406  0.0038562  -0.244    0.807\n",
       "affin            0.0024944  0.0033566   0.743    0.458\n",
       "anger            0.0051489  0.0161589   0.319    0.750\n",
       "anticipation    -0.0115922  0.0095683  -1.212    0.227\n",
       "fear            -0.0059694  0.0123681  -0.483    0.630\n",
       "joy              0.0153111  0.0124422   1.231    0.219\n",
       "negative        -0.0015962  0.0121564  -0.131    0.896\n",
       "positive         0.0107320  0.0085891   1.249    0.212\n",
       "sadness         -0.0146796  0.0142339  -1.031    0.303\n",
       "surprise        -0.0011805  0.0137168  -0.086    0.931\n",
       "disgust         -0.0010644  0.0158255  -0.067    0.946\n",
       "trust           -0.0087859  0.0102697  -0.856    0.393\n",
       "\n",
       "Residual standard error: 0.7028 on 306 degrees of freedom\n",
       "Multiple R-squared:  0.06668,\tAdjusted R-squared:  0.01788 \n",
       "F-statistic: 1.366 on 16 and 306 DF,  p-value: 0.1568\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.753774119311046"
      ],
      "text/latex": [
       "0.753774119311046"
      ],
      "text/markdown": [
       "0.753774119311046"
      ],
      "text/plain": [
       "[1] 0.7537741"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separating training and test data: \n",
    "\n",
    "# Filter out the missing values to get our training data \n",
    "training_data <- transcript_features_df %>%\n",
    "    filter(!is.na(Extr))\n",
    "\n",
    "# head(is.na(training_data)) # check whether the filter worked\n",
    "\n",
    "# select test-data  \n",
    "testset_vloggers <- transcript_features_df %>% \n",
    "    filter(is.na(Extr))\n",
    "\n",
    "# check the filter\n",
    "head(is.na(testset_vloggers))\n",
    "\n",
    "# Full model -------------------------------------------------------------------------------------------------------------------------\n",
    "# predict personality with our features\n",
    "fit_mlm <- lm(cbind(Extr, Agr, Cons, Emot, Open) ~ n_words + word_length + empathy +\n",
    "             distress + total_adjective + affin + anger + anticipation + fear + \n",
    "            joy + negative + positive + sadness + surprise + disgust + trust, data = training_data)\n",
    "\n",
    "summary(fit_mlm) # look at the significance and check model fit\n",
    "sqrt(mean(resid(fit_mlm) ^ 2)) # RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645fa48",
   "metadata": {
    "papermill": {
     "duration": 0.051584,
     "end_time": "2021-09-21T11:02:42.341889",
     "exception": false,
     "start_time": "2021-09-21T11:02:42.290305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predictor Selection: \n",
    "We wanted to assess which features predict well for each personality measure. We chose to do this with a forward stepwise regression. Once the optimal models were chosen, we formed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6ca6a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:42.451386Z",
     "iopub.status.busy": "2021-09-21T11:02:42.449713Z",
     "iopub.status.idle": "2021-09-21T11:02:42.942186Z",
     "shell.execute_reply": "2021-09-21T11:02:42.940649Z"
    },
    "papermill": {
     "duration": 0.54877,
     "end_time": "2021-09-21T11:02:42.942341",
     "exception": false,
     "start_time": "2021-09-21T11:02:42.393571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Extr ~ 1, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.62079 -0.72079  0.07921  0.77921  1.97921 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  4.62079    0.05383   85.84   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.9674 on 322 degrees of freedom\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Extr ~ distress + affin + fear, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.59718 -0.69036  0.02323  0.71732  2.02077 \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -2.158350   1.800140  -1.199 0.231422    \n",
       "distress     2.229566   0.602951   3.698 0.000256 ***\n",
       "affin        0.006307   0.002221   2.839 0.004809 ** \n",
       "fear        -0.012895   0.008344  -1.546 0.123210    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.9463 on 319 degrees of freedom\n",
       "Multiple R-squared:  0.05208,\tAdjusted R-squared:  0.04317 \n",
       "F-statistic: 5.842 on 3 and 319 DF,  p-value: 0.0006777\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.940423984294229"
      ],
      "text/latex": [
       "0.940423984294229"
      ],
      "text/markdown": [
       "0.940423984294229"
      ],
      "text/plain": [
       "[1] 0.940424"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Agr ~ 1, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-2.6966 -0.4966  0.2034  0.6034  1.8034 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  4.69659    0.04958   94.73   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.8911 on 322 degrees of freedom\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Agr ~ disgust + empathy + distress + word_length + \n",
       "    surprise + anger + fear + sadness, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.61039 -0.47677  0.07787  0.49990  1.84859 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.07247    1.67167   0.043  0.96545    \n",
       "disgust     -0.03878    0.01477  -2.625  0.00908 ** \n",
       "empathy      5.01923    0.73986   6.784 5.82e-11 ***\n",
       "distress    -2.86868    0.72720  -3.945 9.85e-05 ***\n",
       "word_length -0.62326    0.23408  -2.663  0.00815 ** \n",
       "surprise     0.03319    0.01020   3.255  0.00126 ** \n",
       "anger       -0.04504    0.01528  -2.947  0.00345 ** \n",
       "fear         0.03534    0.01229   2.876  0.00430 ** \n",
       "sadness     -0.02942    0.01364  -2.157  0.03178 *  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7441 on 314 degrees of freedom\n",
       "Multiple R-squared:   0.32,\tAdjusted R-squared:  0.3027 \n",
       "F-statistic: 18.47 on 8 and 314 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.733659940455756"
      ],
      "text/latex": [
       "0.733659940455756"
      ],
      "text/markdown": [
       "0.733659940455756"
      ],
      "text/plain": [
       "[1] 0.7336599"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Cons ~ 1, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.61494 -0.41494  0.08506  0.48506  1.68506 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  4.51494    0.04393   102.8   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7895 on 322 degrees of freedom\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Agr ~ disgust + empathy + distress + word_length + \n",
       "    surprise + anger + fear + sadness, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.61039 -0.47677  0.07787  0.49990  1.84859 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.07247    1.67167   0.043  0.96545    \n",
       "disgust     -0.03878    0.01477  -2.625  0.00908 ** \n",
       "empathy      5.01923    0.73986   6.784 5.82e-11 ***\n",
       "distress    -2.86868    0.72720  -3.945 9.85e-05 ***\n",
       "word_length -0.62326    0.23408  -2.663  0.00815 ** \n",
       "surprise     0.03319    0.01020   3.255  0.00126 ** \n",
       "anger       -0.04504    0.01528  -2.947  0.00345 ** \n",
       "fear         0.03534    0.01229   2.876  0.00430 ** \n",
       "sadness     -0.02942    0.01364  -2.157  0.03178 *  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7441 on 314 degrees of freedom\n",
       "Multiple R-squared:   0.32,\tAdjusted R-squared:  0.3027 \n",
       "F-statistic: 18.47 on 8 and 314 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.733659940455756"
      ],
      "text/latex": [
       "0.733659940455756"
      ],
      "text/markdown": [
       "0.733659940455756"
      ],
      "text/plain": [
       "[1] 0.7336599"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Emot ~ 1, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-2.5802 -0.3802  0.1198  0.5198  1.7198 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  4.78022    0.04293   111.4   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7715 on 322 degrees of freedom\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Emot ~ disgust + positive + empathy + distress + \n",
       "    sadness + affin + total_adjective, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.23392 -0.41237  0.05529  0.47242  2.02312 \n",
       "\n",
       "Coefficients:\n",
       "                 Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)      2.926520   1.516006   1.930 0.054453 .  \n",
       "disgust         -0.015428   0.014163  -1.089 0.276823    \n",
       "positive         0.009373   0.004607   2.034 0.042755 *  \n",
       "empathy          2.839578   0.703713   4.035 6.86e-05 ***\n",
       "distress        -2.369776   0.694417  -3.413 0.000727 ***\n",
       "sadness         -0.022468   0.011443  -1.963 0.050472 .  \n",
       "affin           -0.006653   0.002885  -2.306 0.021757 *  \n",
       "total_adjective  0.003869   0.002560   1.511 0.131721    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7157 on 315 degrees of freedom\n",
       "Multiple R-squared:  0.1582,\tAdjusted R-squared:  0.1395 \n",
       "F-statistic:  8.46 on 7 and 315 DF,  p-value: 1.72e-09\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.706743372866271"
      ],
      "text/latex": [
       "0.706743372866271"
      ],
      "text/markdown": [
       "0.706743372866271"
      ],
      "text/plain": [
       "[1] 0.7067434"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Open ~ 1, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-2.2659 -0.4659  0.0008  0.5341  1.6341 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  4.66586    0.03946   118.2   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.7092 on 322 degrees of freedom\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Open ~ empathy + fear + joy + anticipation, data = training_data)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-2.25247 -0.48369  0.00258  0.48515  1.51196 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)  \n",
       "(Intercept)   1.844669   1.373313   1.343   0.1802  \n",
       "empathy       0.902493   0.437129   2.065   0.0398 *\n",
       "fear         -0.011812   0.005705  -2.070   0.0392 *\n",
       "joy           0.018940   0.007486   2.530   0.0119 *\n",
       "anticipation -0.012943   0.007593  -1.705   0.0892 .\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 0.6951 on 318 degrees of freedom\n",
       "Multiple R-squared:  0.05125,\tAdjusted R-squared:  0.03932 \n",
       "F-statistic: 4.295 on 4 and 318 DF,  p-value: 0.002129\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.68967562990351"
      ],
      "text/latex": [
       "0.68967562990351"
      ],
      "text/markdown": [
       "0.68967562990351"
      ],
      "text/plain": [
       "[1] 0.6896756"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extr -------------------------------------------------------------------------------------------------------------------------------\n",
    "# empty model \n",
    "fit_empty_extr <- lm(Extr ~ 1, data = training_data)\n",
    "summary(fit_empty_extr)\n",
    "\n",
    "# stepwise \n",
    "fit_extr <- step(fit_empty_extr, direction = \"forward\", scope = Extr ~n_words + word_length + empathy +\n",
    "             distress + total_adjective + affin + anger + anticipation + fear + \n",
    "            joy + negative + positive + sadness + surprise + disgust + trust, data = training_set, trace = 0)\n",
    "summary(fit_extr)\n",
    "\n",
    "#RMSE of Extraversion\n",
    "sqrt(mean(resid(fit_extr) ^ 2))\n",
    "\n",
    "# Agr --------------------------------------------------------------------------------------------------------------------------------\n",
    "fit_empty_agr <- lm(Agr ~ 1, data = training_data)\n",
    "summary(fit_empty_agr)\n",
    "\n",
    "# stepwise \n",
    "fit_agr <- step(fit_empty_agr, direction = \"forward\", scope = Agr ~n_words + word_length + empathy +\n",
    "             distress + total_adjective + affin + anger + anticipation + fear + \n",
    "            joy + negative + positive + sadness + surprise + disgust + trust, data = training_set, trace = 0)\n",
    "summary(fit_agr)\n",
    "\n",
    "#RMSE of Extraversion\n",
    "sqrt(mean(resid(fit_agr) ^ 2))\n",
    "\n",
    "# Cons--------------------------------------------------------------------------------------------------------------------------------\n",
    "fit_empty_cons <- lm(Cons ~ 1, data = training_data)\n",
    "summary(fit_empty_cons)\n",
    "\n",
    "# stepwise \n",
    "fit_cons <- step(fit_empty_agr, direction = \"forward\", scope = Cons ~ n_words + word_length + empathy +\n",
    "             distress + total_adjective + affin + anger + anticipation + fear + \n",
    "            joy + negative + positive + sadness + surprise + disgust + trust, data = training_set, trace = 0)\n",
    "summary(fit_cons)\n",
    "\n",
    "#RMSE of Extraversion\n",
    "sqrt(mean(resid(fit_cons) ^ 2))\n",
    "\n",
    "# Emot -------------------------------------------------------------------------------------------------------------------------------\n",
    "# empty model \n",
    "fit_empty_emot <- lm(Emot~ 1, data = training_data)\n",
    "summary(fit_empty_emot)\n",
    "\n",
    "# stepwise \n",
    "fit_emot <- step(fit_empty_emot, direction = \"forward\", scope = Emot ~n_words + word_length + empathy +\n",
    "             distress + total_adjective + affin + anger + anticipation + fear + \n",
    "            joy + negative + positive + sadness + surprise + disgust + trust, data = training_set, trace = 0)\n",
    "summary(fit_emot)\n",
    "\n",
    "#RMSE of Openess \n",
    "sqrt(mean(resid(fit_emot) ^ 2))\n",
    "\n",
    "# Open -------------------------------------------------------------------------------------------------------------------------------\n",
    "# empty model \n",
    "fit_empty_open <- lm(Open ~ 1, data = training_data)\n",
    "summary(fit_empty_open)\n",
    "\n",
    "# stepwise \n",
    "fit_open <- step(fit_empty_open, direction = \"forward\", scope = Open ~ n_words + word_length + empathy +\n",
    "             distress + total_adjective + affin + anger + anticipation + fear + \n",
    "            joy + negative + positive + sadness + surprise + disgust + trust, data = training_set, trace = 0)\n",
    "summary(fit_open)\n",
    "\n",
    "#RMSE of Openess \n",
    "sqrt(mean(resid(fit_open) ^ 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17317a2e",
   "metadata": {
    "_cell_guid": "d2def0aa-2cf4-47cf-b2f5-31477c6bffda",
    "_uuid": "04dc5694-5fba-4712-b6a0-5e8ea30ded86",
    "papermill": {
     "duration": 0.059267,
     "end_time": "2021-09-21T11:02:43.061408",
     "exception": false,
     "start_time": "2021-09-21T11:02:43.002141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Making predictions on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7c56d",
   "metadata": {
    "_cell_guid": "560128db-06d2-493a-9b78-9fa2ff25881f",
    "_uuid": "d4c0a598-8241-4d91-a60b-cc964318422d",
    "papermill": {
     "duration": 0.05911,
     "end_time": "2021-09-21T11:02:43.180614",
     "exception": false,
     "start_time": "2021-09-21T11:02:43.121504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predictions\n",
    "\n",
    "In this section we make predictions from the models selected through the stepwise regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e947d3e1",
   "metadata": {
    "_cell_guid": "9453537e-1d01-45a9-bfb7-b91b891e7b1e",
    "_uuid": "9263474a-b2a7-4073-88e4-57af25295b0f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:43.305613Z",
     "iopub.status.busy": "2021-09-21T11:02:43.303503Z",
     "iopub.status.idle": "2021-09-21T11:02:43.387408Z",
     "shell.execute_reply": "2021-09-21T11:02:43.385731Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.148056,
     "end_time": "2021-09-21T11:02:43.387571",
     "exception": false,
     "start_time": "2021-09-21T11:02:43.239515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>vlogId</th><th scope=col>Extr</th><th scope=col>Agr</th><th scope=col>Cons</th><th scope=col>Emot</th><th scope=col>Open</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>VLOG8 </td><td>4.569289</td><td>5.121752</td><td>5.121752</td><td>4.735066</td><td>4.602135</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>VLOG15</td><td>4.543831</td><td>4.416545</td><td>4.416545</td><td>4.697841</td><td>4.426627</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>VLOG18</td><td>4.892155</td><td>5.232366</td><td>5.232366</td><td>5.029184</td><td>4.880817</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>VLOG22</td><td>5.389967</td><td>4.629905</td><td>4.629905</td><td>4.783131</td><td>4.991315</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>VLOG28</td><td>4.552347</td><td>4.760180</td><td>4.760180</td><td>4.498248</td><td>4.502251</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>VLOG29</td><td>4.886755</td><td>5.082750</td><td>5.082750</td><td>4.913666</td><td>4.847211</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 6\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & vlogId & Extr & Agr & Cons & Emot & Open\\\\\n",
       "  & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & VLOG8  & 4.569289 & 5.121752 & 5.121752 & 4.735066 & 4.602135\\\\\n",
       "\t2 & VLOG15 & 4.543831 & 4.416545 & 4.416545 & 4.697841 & 4.426627\\\\\n",
       "\t3 & VLOG18 & 4.892155 & 5.232366 & 5.232366 & 5.029184 & 4.880817\\\\\n",
       "\t4 & VLOG22 & 5.389967 & 4.629905 & 4.629905 & 4.783131 & 4.991315\\\\\n",
       "\t5 & VLOG28 & 4.552347 & 4.760180 & 4.760180 & 4.498248 & 4.502251\\\\\n",
       "\t6 & VLOG29 & 4.886755 & 5.082750 & 5.082750 & 4.913666 & 4.847211\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 6\n",
       "\n",
       "| <!--/--> | vlogId &lt;chr&gt; | Extr &lt;dbl&gt; | Agr &lt;dbl&gt; | Cons &lt;dbl&gt; | Emot &lt;dbl&gt; | Open &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 1 | VLOG8  | 4.569289 | 5.121752 | 5.121752 | 4.735066 | 4.602135 |\n",
       "| 2 | VLOG15 | 4.543831 | 4.416545 | 4.416545 | 4.697841 | 4.426627 |\n",
       "| 3 | VLOG18 | 4.892155 | 5.232366 | 5.232366 | 5.029184 | 4.880817 |\n",
       "| 4 | VLOG22 | 5.389967 | 4.629905 | 4.629905 | 4.783131 | 4.991315 |\n",
       "| 5 | VLOG28 | 4.552347 | 4.760180 | 4.760180 | 4.498248 | 4.502251 |\n",
       "| 6 | VLOG29 | 4.886755 | 5.082750 | 5.082750 | 4.913666 | 4.847211 |\n",
       "\n"
      ],
      "text/plain": [
       "  vlogId Extr     Agr      Cons     Emot     Open    \n",
       "1 VLOG8  4.569289 5.121752 5.121752 4.735066 4.602135\n",
       "2 VLOG15 4.543831 4.416545 4.416545 4.697841 4.426627\n",
       "3 VLOG18 4.892155 5.232366 5.232366 5.029184 4.880817\n",
       "4 VLOG22 5.389967 4.629905 4.629905 4.783131 4.991315\n",
       "5 VLOG28 4.552347 4.760180 4.760180 4.498248 4.502251\n",
       "6 VLOG29 4.886755 5.082750 5.082750 4.913666 4.847211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Id</th><th scope=col>Expected</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>VLOG8_Extr </td><td>4.569289</td></tr>\n",
       "\t<tr><td>VLOG8_Agr  </td><td>5.121752</td></tr>\n",
       "\t<tr><td>VLOG8_Cons </td><td>5.121752</td></tr>\n",
       "\t<tr><td>VLOG8_Emot </td><td>4.735066</td></tr>\n",
       "\t<tr><td>VLOG8_Open </td><td>4.602135</td></tr>\n",
       "\t<tr><td>VLOG15_Extr</td><td>4.543831</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Id & Expected\\\\\n",
       " <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t VLOG8\\_Extr  & 4.569289\\\\\n",
       "\t VLOG8\\_Agr   & 5.121752\\\\\n",
       "\t VLOG8\\_Cons  & 5.121752\\\\\n",
       "\t VLOG8\\_Emot  & 4.735066\\\\\n",
       "\t VLOG8\\_Open  & 4.602135\\\\\n",
       "\t VLOG15\\_Extr & 4.543831\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 2\n",
       "\n",
       "| Id &lt;chr&gt; | Expected &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| VLOG8_Extr  | 4.569289 |\n",
       "| VLOG8_Agr   | 5.121752 |\n",
       "| VLOG8_Cons  | 5.121752 |\n",
       "| VLOG8_Emot  | 4.735066 |\n",
       "| VLOG8_Open  | 4.602135 |\n",
       "| VLOG15_Extr | 4.543831 |\n",
       "\n"
      ],
      "text/plain": [
       "  Id          Expected\n",
       "1 VLOG8_Extr  4.569289\n",
       "2 VLOG8_Agr   5.121752\n",
       "3 VLOG8_Cons  5.121752\n",
       "4 VLOG8_Emot  4.735066\n",
       "5 VLOG8_Open  4.602135\n",
       "6 VLOG15_Extr 4.543831"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new predictions from these models \n",
    "pred_extr <- predict(fit_extr, new = testset_vloggers)\n",
    "pred_open <- predict(fit_open, new = testset_vloggers)\n",
    "pred_cons <- predict(fit_cons, new = testset_vloggers)\n",
    "pred_agr <- predict(fit_agr, new = testset_vloggers)\n",
    "pred_emot <- predict(fit_emot, new = testset_vloggers)\n",
    "\n",
    "# combine all the prediction vectors into one data frame \n",
    "testset_final_pred  = testset_vloggers %>% \n",
    "    mutate(\n",
    "        Extr = pred_extr, \n",
    "        Agr  = pred_agr,\n",
    "        Cons = pred_cons,\n",
    "        Emot = pred_emot,\n",
    "        Open = pred_open\n",
    "    ) %>%\n",
    "    select(vlogId, Extr:Open)\n",
    "\n",
    "head(testset_final_pred )\n",
    "\n",
    "\n",
    "\n",
    "# convert to competition format: 400 rows \n",
    "testset_final_pred <- testset_final_pred %>%\n",
    "    as_tibble() %>%\n",
    "    pivot_longer(c(Extr, Agr, Cons, Emot, Open), names_to = 'personality', values_to = 'Expected') \n",
    "testset_final_pred <- testset_final_pred  %>%\n",
    "    unite(Id, vlogId, personality)\n",
    "\n",
    "head(testset_final_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2387b",
   "metadata": {
    "_cell_guid": "4528893d-1930-4ccf-b4c0-51a0c773dae5",
    "_uuid": "2ef3a62f-62cf-42ca-9f50-c07a93d04ccf",
    "papermill": {
     "duration": 0.06127,
     "end_time": "2021-09-21T11:02:43.509734",
     "exception": false,
     "start_time": "2021-09-21T11:02:43.448464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 Writing predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a78dd486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T11:02:43.637933Z",
     "iopub.status.busy": "2021-09-21T11:02:43.636213Z",
     "iopub.status.idle": "2021-09-21T11:02:43.659933Z",
     "shell.execute_reply": "2021-09-21T11:02:43.658398Z"
    },
    "papermill": {
     "duration": 0.088997,
     "end_time": "2021-09-21T11:02:43.660119",
     "exception": false,
     "start_time": "2021-09-21T11:02:43.571122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'__notebook__.ipynb'</li><li>'AFINN'</li><li>'afinn.zip'</li><li>'nrc.txt'</li><li>'predictions.final.csv'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '\\_\\_notebook\\_\\_.ipynb'\n",
       "\\item 'AFINN'\n",
       "\\item 'afinn.zip'\n",
       "\\item 'nrc.txt'\n",
       "\\item 'predictions.final.csv'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '__notebook__.ipynb'\n",
       "2. 'AFINN'\n",
       "3. 'afinn.zip'\n",
       "4. 'nrc.txt'\n",
       "5. 'predictions.final.csv'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"__notebook__.ipynb\"    \"AFINN\"                 \"afinn.zip\"            \n",
       "[4] \"nrc.txt\"               \"predictions.final.csv\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write to csv file \n",
    "write.csv(testset_final_pred, \"predictions.final.csv\")\n",
    "\n",
    "# Check if the file was written successfully.\n",
    "list.files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb11e0",
   "metadata": {
    "_cell_guid": "d440c5a9-2554-4618-bdae-ae2625df13ef",
    "_uuid": "ceb46677-aa88-4b59-b6fe-1a6ea5afd432",
    "papermill": {
     "duration": 0.062307,
     "end_time": "2021-09-21T11:02:43.784991",
     "exception": false,
     "start_time": "2021-09-21T11:02:43.722684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once you have clicked the <span style=\"background-color:#000000;color:white;padding:3px;border-radius:10px;padding-left:6px;padding-right:6px;\">⟳ Save Version&nbsp;&nbsp;|&nbsp;&nbsp;0</span> button at the top left, and select the \"Save & Run All (Commit)\" option, go to the Viewer. There you will find your \"predictions.csv\" under Output. You'll also see a button there that allows you to submit your predictions with one click."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6f323",
   "metadata": {
    "papermill": {
     "duration": 0.062749,
     "end_time": "2021-09-21T11:02:43.909438",
     "exception": false,
     "start_time": "2021-09-21T11:02:43.846689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Best Model based on RMSE: \n",
    "Our best model looked as follows: the four predictors were n_words + word_length + empathy + distress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.621353,
   "end_time": "2021-09-21T11:02:44.081854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-21T11:02:25.460501",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
